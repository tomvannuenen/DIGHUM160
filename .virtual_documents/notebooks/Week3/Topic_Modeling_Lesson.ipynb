








import pandas as pd
import numpy as np

df = pd.read_csv('../../data/aita_pp.csv')





lemmas_split = [lemma.split() for lemma in df['pp_text']]


lemmas_split[0][:10]





from gensim import corpora, models, similarities
from gensim.models.coherencemodel import CoherenceModel

# Create Dictionary 
dictionary = corpora.Dictionary(lemmas_split)

# filter extremes and assign new ids
dictionary.filter_extremes(no_below=10, no_above=0.4)
dictionary.compactify() 

# SAVE DICT
dictionary.save('../../data/aita_lda.dict')

# Create Document-Term Matrix of our whole corpus 
corpus = [dictionary.doc2bow(text) for text in lemmas_split]





corpus[0][:10]





dictionary[6]





dictionary.token2id['father']





from gensim.models.ldamodel import LdaModel

%time
lda_model = LdaModel(corpus=corpus,   # stream of document vectors or sparse matrix of shape
            id2word=dictionary,       # mapping from word IDs to words (for determining vocab size)
            num_topics=10,            # amount of topics
            random_state=100,         # seed to generate random state; useful for reproducibility
            passes=2,                 # amount of iterations/epochs 
            per_word_topics=False)    # computing most-likely topics for each word 








#%pip install pyLDAvis "numpy<2.0"





import pyLDAvis
import pyLDAvis.gensim_models as gensimvis
pyLDAvis.enable_notebook()

# feed the LDA model into the pyLDAvis instance
lda_viz = gensimvis.prepare(lda_model, corpus, dictionary)
lda_viz








import warnings
warnings.simplefilter("ignore", DeprecationWarning)

import spacy
#!spacy download en_core_web_sm
nlp = spacy.load('en_core_web_sm')

def POS(text, allowed_postags = ['NOUN', 'ADJ']):
    parsed = nlp(text)
    return [token.lemma_ for token in parsed if token.pos_ in allowed_postags]


# turn our POS tagged lemmas into a string so we can save them in our DF
str_pos_lemmas = [' '.join(t) for t in lemmas_split]


# This will take a long time
from tqdm import tqdm

pos_lemmas_split = [POS(text) for text in tqdm(df['pp_text'])]





# Create Dictionary 
pos_dictionary = corpora.Dictionary(tqdm(pos_lemmas_split))

# filter extremes and assign new ids
pos_dictionary.filter_extremes(no_below=10, no_above=0.4)
pos_dictionary.compactify() 

# SAVE DICT
pos_dictionary.save('../../data/aita_pos_lda.dict')

# Create Document-Term Matrix of our whole corpus 
pos_corpus = [pos_dictionary.doc2bow(text) for text in tqdm(pos_lemmas_split)]






import logging
for handler in logging.root.handlers[:]:
    logging.root.removeHandler(handler)

logging.basicConfig(filename='../../data/gensim.log', filemode='w', format="%(asctime)s:%(levelname)s:%(message)s", level=logging.INFO)

lda_model_tweak = LdaModel(corpus=pos_corpus,
                           id2word=pos_dictionary,
                           num_topics=20, 
                           random_state=100,
                           eval_every=1,           # show perplexity after every update for visualization
                           passes=5,               # number of model training cycles, aka epochs
                           per_word_topics=False)





import matplotlib.pyplot as plt
%matplotlib inline
import re

p = re.compile(r"(-*\d+\.\d+) per-word .* (\d+\.\d+) perplexity")
matches = [p.findall(l) for l in open('../../data/gensim.log')]
matches = [m for m in matches if len(m) > 0]
tuples = [t[0] for t in matches]
likelihood = [float(t[0]) for t in tuples]
perplexity = [float(t[1]) for t in tuples]
iter = list(range(0,len(tuples)*10,10))
plt.plot(iter,likelihood,c="black")
plt.ylabel("log likelihood")
plt.xlabel("iteration")
plt.title("Topic Model Convergence")
plt.grid()








#import logging
#logging.getLogger().setLevel(logging.CRITICAL)
import warnings
warnings.simplefilter("ignore", DeprecationWarning)

# Compute Coherence Score
coherence_model = CoherenceModel(model=lda_model_tweak, corpus=pos_corpus, texts=tqdm(pos_lemmas_split), dictionary=pos_dictionary, coherence='c_v') 
coherence = coherence_model.get_coherence()
print('\nCoherence Score: ', coherence)








from gensim.models.ldamodel import LdaModel
from gensim.models.coherencemodel import CoherenceModel
from tqdm import tqdm

def compute_coherence_values(dictionary, corpus, texts, start=5, limit=20, step=3):
    """
    Compute c_v coherence for various number of topics
    """
    coherence_values = []
    model_list = []
    
    for num_topics in tqdm(range(start, limit, step), desc="Training LDA models"):
        # Train LDA model
        model = LdaModel(
            corpus=corpus, 
            id2word=dictionary, 
            num_topics=num_topics, 
            random_state=100,
            passes=5,
            alpha='auto'
        )
        
        # Calculate coherence
        coherence_model = CoherenceModel(
            model=model, 
            texts=texts, 
            dictionary=dictionary, 
            coherence='c_v'
        )
        
        model_list.append(model)
        coherence_values.append(coherence_model.get_coherence())
    
    return model_list, coherence_values





# Run the function
model_list, coherence_values = compute_coherence_values(
    dictionary=pos_dictionary, 
    corpus=pos_corpus, 
    texts=pos_lemmas_split
)





import matplotlib.pyplot as plt
%matplotlib inline

# Show graph
start=5; limit=20; step=3
x = range(start, limit, step) # range uses start, stop, and incrementation
plt.plot(x, coherence_values)
plt.xlabel("Num Topics")
plt.ylabel("Coherence score")
plt.legend(("coherence_values"), loc='best')
plt.show()


# Print these coherence scores
c = 0
for m, cv in zip(x, coherence_values):
    print(f"model_list[{c}]: Num Topics = {m}, Coherence Value = {round(cv, 4)}")
    c += 1





from gensim import corpora, models, similarities

# SAVE MODEL
optimal_lda_model = model_list[1]
optimal_lda_model.save('../../data/aita_pos_lda_optimal.model')



import pyLDAvis
import pyLDAvis.gensim_models as gensimvis
pyLDAvis.enable_notebook()

# feed the LDA model into the pyLDAvis instance
lda_viz = gensimvis.prepare(optimal_lda_model, pos_corpus, pos_dictionary)
lda_viz





# LOAD MODEL
optimal_lda_model = LdaModel.load('../../data/aita_pos_lda_optimal.model')

# LOAD DICT
pos_dictionary = corpora.Dictionary.load('../../data/aita_pos_lda.dict')

# LOAD CORPUS
pos_corpus = [pos_dictionary.doc2bow(text) for text in tqdm(pos_lemmas_split)]





from pprint import pprint

# Select the ideal model and print the topics
model_topics = optimal_lda_model.show_topics(formatted=False)
pprint(optimal_lda_model.print_topics(num_words=20))





# giving names to our topics
topic_names = {0: 'home, everyday life', 
               1: 'celebration, relationships',
               2: 'family',
               3: 'school, work',
               4: 'having kids',
               5: 'expenses',
               6: 'dating',
               7: 'appropriateness'
              }








def dominant_topic(ldamodel=optimal_lda_model, corpus=corpus, texts=df['selftext']):
    # Create DF
    topics_df = pd.DataFrame()

    # Get main topic in each document
    for i, row in enumerate(ldamodel[corpus]):
        row = sorted(row, key=lambda x: (x[1]), reverse=True)
        # Get the Dominant topic, Perc Contribution and Keywords for each thread
        for j, (topic_num, prop_topic) in enumerate(row):
            if j == 0:  # => dominant topic
                wp = ldamodel.show_topic(topic_num)
                # get value of topic_names dict based on key
                topic_name = topic_names[topic_num]
                topic_keywords = ", ".join([word for word, prop in wp])
                new_row = pd.DataFrame([[int(topic_num), topic_name, round(prop_topic,4), topic_keywords]])
                topics_df = pd.concat([topics_df, new_row], ignore_index=True)

            else:
                break
    topics_df.columns = ['Dominant_Topic', 'Dominant_Topic_Name', 'Perc_Contribution', 'Topic_Keywords']

    # Add original text to the end of the output
    contents = pd.Series(texts)
    topics_df = pd.concat([topics_df, contents], axis=1)
    return topics_df 

# Run function
df_topic_keywords = dominant_topic(ldamodel=optimal_lda_model, corpus=pos_corpus, texts=df['selftext'])

# Format
df_dominant_topic = df_topic_keywords.reset_index(drop=True)
df_dominant_topic.columns = ['Dominant_Topic', 'Dominant_Topic_Name', 'Topic_Perc_Contrib', 'Keywords', 'Text']

# Show
df_dominant_topic





df_dominant_topic.loc[df_dominant_topic['Dominant_Topic_Name'] == 'expenses']['Text']





df_dominant_topic.loc[df_dominant_topic['Dominant_Topic_Name'] == 'expenses']['Text'][6]








df['dom_topic'] = df_dominant_topic['Dominant_Topic_Name']
df['dom_topic_num'] = df_dominant_topic['Dominant_Topic']


df.head(3)





# Number of topics
num_topics = optimal_lda_model.num_topics

# Initialize the overlap matrix
overlap_matrix = np.zeros((num_topics, num_topics))

# Iterate through documents and get topic probabilities
for document in tqdm(pos_corpus, desc="Processing documents"):
    document_topics = optimal_lda_model.get_document_topics(document, minimum_probability=0)
    # Create a full topic distribution for the document
    full_topic_distribution = [0] * num_topics
    for topic_num, prob in document_topics:
        full_topic_distribution[topic_num] = prob

    # Iterate through pairs of topics and add probabilities to the overlap matrix
    for i in range(num_topics):
        for j in range(num_topics):
            overlap_matrix[i, j] += full_topic_distribution[i] * full_topic_distribution[j]

# Normalize the overlap matrix by dividing by the number of documents
overlap_matrix /= len(pos_corpus)

# Assuming overlap_matrix is your overlap matrix
np.fill_diagonal(overlap_matrix, 0)

# Now apply the threshold
co_occurrence_matrix = np.where(overlap_matrix > 0.01, overlap_matrix, 0)



import networkx as nx

# Create a graph from the co-occurrence matrix
G = nx.Graph()
for i in range(co_occurrence_matrix.shape[0]):
    for j in range(co_occurrence_matrix.shape[1]):
        if co_occurrence_matrix[i, j] > 0:
            G.add_edge(i, j, weight=co_occurrence_matrix[i, j])

# Define node colors based on the number of links remaining after removal
node_colors = [len(list(G.neighbors(n))) for n in G.nodes()]

# Define edge colors based on co-occurrence strength
edge_colors = [d['weight'] for _, _, d in G.edges(data=True)]

plt.figure(figsize=(12, 12))
pos = nx.spring_layout(G)  # Can use other layouts as well

# Define an offset for the labels
label_offset = 0.05

# Create a new dictionary for the label positions
label_pos = {node: (coordinates[0], coordinates[1] + label_offset) for node, coordinates in pos.items()}

nx.draw_networkx_nodes(G, pos, node_color=node_colors, cmap=plt.cm.Reds)
nx.draw_networkx_edges(G, pos, edge_color=edge_colors, edge_cmap=plt.cm.Blues)
nx.draw_networkx_labels(G, label_pos, labels=topic_names)

plt.show()







