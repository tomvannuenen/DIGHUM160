{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bhs6S4deZFWW"
   },
   "source": [
    "# Topic Modeling – Project Notebook\n",
    "\n",
    "Use this notebook for carrying out the analyses from the workshop notebook on your own subreddit data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ib68CU9JeGgc"
   },
   "source": [
    "## Loading the data\n",
    "\n",
    "Make sure to use the preprocessed file from Week 1, with the `lemmas` column in it!\n",
    "\n",
    "In all of the cells below, replace YOUR_FILE with the name of the files you are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2574,
     "status": "ok",
     "timestamp": 1639939141559,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "jlWzV3rveIdK",
    "outputId": "f6f0d10b-b6ff-4236-aaa4-6ec1cc29e980"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../../data/YOUR_FILE_PP.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "executionInfo": {
     "elapsed": 313,
     "status": "ok",
     "timestamp": 1639924447761,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "dkpQ_rR7HSyV",
    "outputId": "027347ac-5e0b-4d61-9a35-cd00c277ff77"
   },
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all rows that are '[removed]' or '[deleted]'\n",
    "df = df.loc[~df['pp_text'].isin(['[removed]', '[deleted]' ]),:]\n",
    "\n",
    "# Select only rows that have >3 characters in selftext\n",
    "df = df.loc[df['pp_text'].str.len() > 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JitcKXJqffVc"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "lemmas_split = [lemma.split() for lemma in tqdm(df['pp_text'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNmgAQEoZWzx"
   },
   "source": [
    "## Creating a `Dictionary` with Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v5U9p6rrZWzy"
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Create Dictionary \n",
    "dictionary = corpora.Dictionary(tqdm(lemmas_split))\n",
    "\n",
    "# filter extremes and assign new ids\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.4)\n",
    "dictionary.compactify() \n",
    "\n",
    "# SAVE DICT\n",
    "dictionary.save('../../data/YOUR_FILE.dict')\n",
    "\n",
    "# Create Document-Term Matrix of our whole corpus \n",
    "corpus = [dictionary.doc2bow(text) for text in tqdm(lemmas_split)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uep-_3ImZW0S"
   },
   "source": [
    "# Running a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28047,
     "status": "ok",
     "timestamp": 1639913487603,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "GV46eCt2ydXL",
    "outputId": "5af4d56d-7564-40a8-d4ec-739f9778ce9c"
   },
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "%time\n",
    "lda_model = LdaModel(corpus=tqdm(corpus),   # stream of document vectors or sparse matrix of shape\n",
    "            id2word=dictionary,       # mapping from word IDs to words (for determining vocab size)\n",
    "            num_topics=10,            # amount of topics\n",
    "            random_state=100,         # seed to generate random state; useful for reproducibility\n",
    "            passes=2,                 # amount of iterations/epochs \n",
    "            per_word_topics=False)    # computing most-likely topics for each word "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vis'></a>\n",
    "\n",
    "# Visualizing a Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 992
    },
    "executionInfo": {
     "elapsed": 33843,
     "status": "ok",
     "timestamp": 1639933951198,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "RiccfvG7IuRZ",
    "outputId": "e02afe24-c09f-4b39-ee1e-5d93d3f0fbfb"
   },
   "outputs": [],
   "source": [
    "# In case you need to install pyLDAvis\n",
    "%pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 916
    },
    "executionInfo": {
     "elapsed": 10987,
     "status": "ok",
     "timestamp": 1639940889440,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "Aze6E-0ZZW0e",
    "outputId": "81b759c9-9655-421f-d6e7-0648a7a42802"
   },
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# feed the LDA model into the pyLDAvis instance\n",
    "lda_viz = gensimvis.prepare(lda_model, corpus, dictionary)\n",
    "lda_viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jOGnQ-WiZW0h"
   },
   "source": [
    "On the left, there is a 2D plot of the \"distance\" between all of the topics (labeled as the Intertopic Distance Map). This plot uses a multidimensional scaling (MDS) algorithm. \n",
    "- Similar topics should appear close together on the plot; dissimilar topics should appear far apart. \n",
    "- The relative size of a topic's circle in the plot corresponds to the relative frequency of the topic in the corpus.\n",
    "\n",
    "### Exploring topics and words\n",
    "- You can scrutinize a topic more closely by clicking on its circle, or entering its number in the \"selected topic\" box in the upper-left (Note that, though the data used by gensim and pyLDAvis are the same, they don't use the same ID numbers for topics.)\n",
    "- If you roll your mouse over a term in the bar chart on the right, the topic circles will resize in the plot on the left. This shows the strength of the relationship between the topics and the selected term.\n",
    "\n",
    "### Salience\n",
    "On the right, there is a bar chart with the top terms. When no topic is selected in the plot on the left, the bar chart shows the top-30 most **salient** terms in the corpus. A term's saliency is a measure of both how frequent the term is in the corpus and how \"distinctive\" it is in distinguishing between different topics.\n",
    "\n",
    "### Probability Vs Exclusivity \n",
    "When you select a particular topic, this bar chart changes to show the top-30 most \"relevant\" terms for the selected topic. The relevance metric is controlled by the parameter λ, which can be adjusted with a slider above the bar chart:\n",
    "\n",
    "* Setting λ close to 1.0 (the default) will rank the terms according to their probability within the topic.\n",
    "* Setting λ close to 0.0 will rank the terms according to their \"distinctiveness\" or \"exclusivity\" within the topic. This means that terms that occur only in this topic, and do not occur in other topics.\n",
    "\n",
    "You can move the slider between 0.0 and 1.0 to weigh term probability and exclusivity.\n",
    "\n",
    "### Exploring the graph\n",
    "The interactive visualization pyLDAvis produces is helpful for **individual** topics: you can manually select each topic to view its top most frequent and/or \"relevant\" terms, using different values of the λ parameter. This can help when you're trying to assign a name or \"meaning\" to each topic. \n",
    "\n",
    "It also helps you to see the **relationships** between topics: exploring the Intertopic Distance Plot can help you learn about how topics relate to each other, including potential higher-level structure between groups of topics.\n",
    "\n",
    "### Getting insights about the model\n",
    "As you can see, this model probably has too many topics: they are overlapping, and most of them appear in one corner of the graph. So we have our first hint that we might want to alter our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osn95IYrZW0i"
   },
   "source": [
    "<a id='coh'></a>\n",
    "\n",
    "# Calculating Topic Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43064,
     "status": "ok",
     "timestamp": 1639913530642,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "wQFU0eNUZW0i",
    "outputId": "9de0d9f7-993b-4228-b6a7-0bbc8e25ffd5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import logging\n",
    "#logging.getLogger().setLevel(logging.CRITICAL)\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model = CoherenceModel(model=lda_model, corpus=corpus, texts=tqdm(lemmas_split), dictionary=dictionary, coherence='c_v') \n",
    "coherence = coherence_model.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BwWnkvExc11X"
   },
   "source": [
    "## Tweaking the data - POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WZtZ7YtScsMX"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "\n",
    "import spacy\n",
    "#!spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def POS(text, allowed_postags = ['NOUN', 'ADJ']):\n",
    "    parsed = nlp(text)\n",
    "    return [token.lemma_ for token in parsed if token.pos_ in allowed_postags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take a long time\n",
    "pos_lemmas_split = [POS(text) for text in tqdm(df['pp_text'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eKHxNE4LbK7A"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../../data/YOUR_FILE_pos_lemmas.json', 'w' ) as write:\n",
    "    json.dump(pos_lemmas_split, write)\n",
    "\n",
    "# Uncomment the following two lines if you want to import this data again\n",
    "#with open(\"aita_pos_lemmas.json\") as f:\n",
    "#    pos_lemmas = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn them into a string so we can save them in our DF\n",
    "str_pos_lemmas = [' '.join(t) for t in pos_lemmas_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_pos_lemmas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pos_lemmas'] = str_pos_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../../data/YOUR_FILE_pos_lemmas.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new dictionary and corpus objects for Gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary \n",
    "pos_dictionary = corpora.Dictionary(tqdm(pos_lemmas_split))\n",
    "\n",
    "# filter extremes and assign new ids\n",
    "pos_dictionary.filter_extremes(no_below=10, no_above=0.4)\n",
    "pos_dictionary.compactify() \n",
    "\n",
    "# SAVE DICT\n",
    "pos_dictionary.save('../../data/YOUR_FILE_pos_lda.dict')\n",
    "\n",
    "# Create Document-Term Matrix of our whole corpus \n",
    "pos_corpus = [pos_dictionary.doc2bow(text) for text in tqdm(pos_lemmas_split)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1sP9L4KPXyXm"
   },
   "source": [
    "## Tweaking hyperparameters\n",
    "\n",
    "`passes` controls how often we train the model on the entire corpus. Another word for passes might be “epochs”.\n",
    " \n",
    "`iterations` is somewhat technical, but essentially it controls how often we repeat a particular loop over each document. It is important to set the number of “passes” and “iterations” high enough.\n",
    "\n",
    "Gensim's designer suggests the following way to choose iterations and passes. First, enable `logging`, and set `eval_every = 1` in `LdaModel`. This will yield a **perplexity** score for every update. Perplexity is a measure of how well a probability model predicts a sample. It captures how surprised a model is of new data it has not seen before, and is measured as the normalized log-likelihood of a held-out test set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ufxXZDJMYKB2"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(filename='../../data/gensim_my_project.log', filemode='w', format=\"%(asctime)s:%(levelname)s:%(message)s\", level=logging.INFO)\n",
    "\n",
    "lda_model_tweak = LdaModel(corpus=tqdm(pos_corpus),\n",
    "                           id2word=pos_dictionary,\n",
    "                           num_topics=20, \n",
    "                           random_state=100,\n",
    "                           eval_every=1,           # show perplexity after every update for visualization\n",
    "                           iterations=50,          # number of model iterations over each doc\n",
    "                           passes=20,              # number of model training cycles, aka epochs\n",
    "                           per_word_topics=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jgq0gBoOTNha"
   },
   "source": [
    "Search through our newly created \"gensim.log\" file and find  / plot the relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QTjA3CXoeBNg"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "\n",
    "p = re.compile(r\"(-*\\d+\\.\\d+) per-word .* (\\d+\\.\\d+) perplexity\")\n",
    "matches = [p.findall(l) for l in open('../../data/gensim_my_project.log')]\n",
    "matches = [m for m in matches if len(m) > 0]\n",
    "tuples = [t[0] for t in matches]\n",
    "likelihood = [float(t[0]) for t in tuples]\n",
    "perplexity = [float(t[1]) for t in tuples]\n",
    "iter = list(range(0,len(tuples)*10,10))\n",
    "plt.plot(iter,likelihood,c=\"black\")\n",
    "plt.ylabel(\"log likelihood\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.title(\"Topic Model Convergence\")\n",
    "plt.grid()\n",
    "plt.savefig(\"convergence_liklihood_1_it_1_pa.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RjFdiMk6ZW0v"
   },
   "source": [
    "## Changing number of topics\n",
    "\n",
    "This `compute_coherence_values()` function trains multiple LDA models, provides the models, and tells you their corresponding coherence scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oOZRa_4LZW0w"
   },
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "def compute_coherence_values(dictionary, corpus, texts, start, limit, step):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : Tokenized text (list of lis of str)\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    passes = 5\n",
    "    for num_topics in tqdm(range(start, limit, step)):\n",
    "        model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=100, \n",
    "                         update_every=1, passes=passes, alpha='auto', per_word_topics=False)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        # When using 'c_v' texts should be provided, corpus isn’t needed. \n",
    "        # When using ‘u_mass’ corpus should be provided, if texts is provided, it will be converted to corpus using the dictionary \n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 242071,
     "status": "ok",
     "timestamp": 1639940656827,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "LYWcr2NhZW0z",
    "outputId": "0f0f4a01-b509-4c46-d11d-015179e38522"
   },
   "outputs": [],
   "source": [
    "# Can take a long time to run\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=pos_dictionary, \n",
    "                                                        corpus=pos_corpus, texts=pos_lemmas_split, \n",
    "                                                        start=6, limit=30, step=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ks6jTrgtpC1m"
   },
   "source": [
    "Visualize the output of the coherence scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 901,
     "status": "ok",
     "timestamp": 1639940737397,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "td7Zi7YpZW03",
    "outputId": "dee7e449-79a6-461b-c582-dba9eeb9cef6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Show graph\n",
    "start=6; limit=30; step=5\n",
    "x = range(start, limit, step) # range uses start, stop, and incrementation\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 548,
     "status": "ok",
     "timestamp": 1639940764419,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "2zX73ICjZW05",
    "outputId": "e1593fa6-dcd7-4bf9-eb75-5b838e238b10",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print these coherence scores\n",
    "c = 0\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(f\"model_list[{c}]: Num Topics = {m}, Coherence Value = {round(cv, 4)}\")\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPwvgE04ZW0_"
   },
   "source": [
    "If the coherence score seems to keep increasing, it generally makes sense to pick the model that gave the highest CV before dropping again.\n",
    "\n",
    "Replace MY_MODEL below with the number of the model that achieves the best results for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UzmroqZHm3M3"
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "\n",
    "# SAVE MODEL\n",
    "optimal_lda_model = model_list[MY_MODEL]\n",
    "optimal_lda_model.save('../../data/aita_pos_lda_optimal.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# feed the LDA model into the pyLDAvis instance\n",
    "lda_viz = gensimvis.prepare(optimal_lda_model, pos_corpus, pos_dictionary)\n",
    "lda_viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you want to load these models from disk again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD MODEL\n",
    "optimal_lda_model = LdaModel.load('../../data/YOUR_FILE_pos_lda_optimal.model')\n",
    "\n",
    "# LOAD DICT\n",
    "pos_dictionary = corpora.Dictionary.load('../../data/YOUR_FILE_pos_lda.dict')\n",
    "\n",
    "# LOAD CORPUS\n",
    "pos_corpus = [pos_dictionary.doc2bow(text) for text in tqdm(pos_lemmas_split)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naming our topics\n",
    "\n",
    "The next thing we should do is name our topics. This is the most important interpretative step in the process: after all, our model has no semantic knowledge of the data. We will print out the top words of each topic, then go over all of them and give them names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 325,
     "status": "ok",
     "timestamp": 1639940904382,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "h3IDZcmfZW1A",
    "outputId": "9eff0a15-579a-4d68-afb7-e29d7f3d6bb8"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Select the ideal model and print the topics\n",
    "model_topics = optimal_lda_model.show_topics(formatted=False)\n",
    "pprint(optimal_lda_model.print_topics(num_words=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name the topics you have. Make sure to elongate / shorten this dictionary based on how many topics you have in your final topic model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VWep2BUPAc97"
   },
   "outputs": [],
   "source": [
    "# giving names to our topics; remove or add as needed\n",
    "\n",
    "topic_names = {0: 'NAME ME', \n",
    "               1: 'NAME ME', \n",
    "               2: 'NAME ME', \n",
    "               3: 'NAME ME', \n",
    "               4: 'NAME ME', \n",
    "               5: 'NAME ME', \n",
    "               6: 'NAME ME',\n",
    "               7: 'NAME ME', \n",
    "               8: 'NAME ME', \n",
    "               9: 'NAME ME', \n",
    "               10: 'NAME ME', \n",
    "               11: 'NAME ME', \n",
    "               12: 'NAME ME', \n",
    "               13: 'NAME ME', \n",
    "               14: 'NAME ME', \n",
    "               15: 'NAME ME'\n",
    "              } \n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naming topics is a heavily iterative process, based on looking closer at particular posts (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fK6uo1EZW1M"
   },
   "source": [
    "<a id='use'></a>\n",
    "\n",
    "# Using Topic Models: What is a Reddit Post About?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "executionInfo": {
     "elapsed": 118204,
     "status": "ok",
     "timestamp": 1639923042784,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "iz_TBN_dZW1Q",
    "outputId": "49fef3dd-409f-444e-dc75-fd443d24ca0f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dominant_topic(ldamodel=optimal_lda_model, corpus=corpus, texts=df['selftext']):\n",
    "    topics_data = []\n",
    "    \n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        \n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each thread\n",
    "        topic_num, prop_topic = row[0]\n",
    "        wp = ldamodel.show_topic(topic_num)\n",
    "        topic_name = topic_names[topic_num]\n",
    "        topic_keywords = \", \".join([word for word, prop in wp])\n",
    "        \n",
    "        topics_data.append({'Dominant_Topic': topic_num, \n",
    "                            'Dominant_Topic_Name': topic_name, \n",
    "                            'Perc_Contribution': round(prop_topic,4), \n",
    "                            'Topic_Keywords': topic_keywords})\n",
    "    \n",
    "    # Create DataFrame\n",
    "    topics_df = pd.DataFrame(topics_data)\n",
    "    \n",
    "    # Add original text to the end of the output\n",
    "    topics_df = pd.concat([topics_df, texts.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    return topics_df \n",
    "\n",
    "# Run function\n",
    "df_dominant_topic = dominant_topic(ldamodel=optimal_lda_model, corpus=pos_corpus, texts=df['selftext'])\n",
    "\n",
    "# Show\n",
    "df_dominant_topic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_OLtA9Wtq2F"
   },
   "source": [
    "We can now find the posts with a dominant topic using `.loc`.\n",
    "\n",
    "Change 'TOPIC NAME' below to a topic you have named above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1639923059054,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "bQBSqG5QtSud",
    "outputId": "ed2a055a-ca20-4e9d-9717-7a13000ab208"
   },
   "outputs": [],
   "source": [
    "df_dominant_topic.loc[df_dominant_topic['Dominant_Topic_Name'] == 'TOPIC NAME']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding topics to original DF\n",
    "\n",
    "Once we are happy with your topic names, we can add the dominant topic names to our original DataFrame and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dom_topic'] = df_dominant_topic['Dominant_Topic_Name']\n",
    "df['dom_topic_num'] = df_dominant_topic['Dominant_Topic']\n",
    "\n",
    "df.to_csv('../../data/YOUR_FILE_pos_lemmas_topics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Co-occurrence\n",
    "\n",
    "We only added the \"dominant topic\" (i.e., the topic with the highest probability) to our DataFrame, but we should remember that topic models assign probabilities for all topics across all documents. \n",
    "\n",
    "This means we could also create a network graph that displays co-occurring topics. For each document, we check the topics This is especially helpful if you have named your topics, as it allows you to see which of the themes frequently seem to occur together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of topics\n",
    "num_topics = optimal_lda_model.num_topics\n",
    "\n",
    "# Initialize the overlap matrix\n",
    "overlap_matrix = np.zeros((num_topics, num_topics))\n",
    "\n",
    "# Iterate through documents and get topic probabilities\n",
    "for document in tqdm(pos_corpus, desc=\"Processing documents\"):\n",
    "    document_topics = optimal_lda_model.get_document_topics(document, minimum_probability=0)\n",
    "    # Create a full topic distribution for the document\n",
    "    full_topic_distribution = [0] * num_topics\n",
    "    for topic_num, prob in document_topics:\n",
    "        full_topic_distribution[topic_num] = prob\n",
    "\n",
    "    # Iterate through pairs of topics and add probabilities to the overlap matrix\n",
    "    for i in range(num_topics):\n",
    "        for j in range(num_topics):\n",
    "            overlap_matrix[i, j] += full_topic_distribution[i] * full_topic_distribution[j]\n",
    "\n",
    "# Normalize the overlap matrix by dividing by the number of documents\n",
    "overlap_matrix /= len(pos_corpus)\n",
    "\n",
    "# Assuming overlap_matrix is your overlap matrix\n",
    "np.fill_diagonal(overlap_matrix, 0)\n",
    "\n",
    "# Now apply the threshold\n",
    "co_occurrence_matrix = np.where(overlap_matrix > 0.01, overlap_matrix, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a graph from the co-occurrence matrix\n",
    "G = nx.Graph()\n",
    "for i in range(co_occurrence_matrix.shape[0]):\n",
    "    for j in range(co_occurrence_matrix.shape[1]):\n",
    "        if co_occurrence_matrix[i, j] > 0:\n",
    "            G.add_edge(i, j, weight=co_occurrence_matrix[i, j])\n",
    "\n",
    "# Define node colors based on the number of links remaining after removal\n",
    "node_colors = [len(list(G.neighbors(n))) for n in G.nodes()]\n",
    "\n",
    "# Define edge colors based on co-occurrence strength\n",
    "edge_colors = [d['weight'] for _, _, d in G.edges(data=True)]\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "pos = nx.spring_layout(G)  # Can use other layouts as well\n",
    "\n",
    "# Define an offset for the labels\n",
    "label_offset = 0.05\n",
    "\n",
    "# Create a new dictionary for the label positions\n",
    "label_pos = {node: (coordinates[0], coordinates[1] + label_offset) for node, coordinates in pos.items()}\n",
    "\n",
    "nx.draw_networkx_nodes(G, pos, node_color=node_colors, cmap=plt.cm.Reds)\n",
    "nx.draw_networkx_edges(G, pos, edge_color=edge_colors, edge_cmap=plt.cm.Blues)\n",
    "nx.draw_networkx_labels(G, label_pos, labels=topic_names)\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Week 3-1 Topic Modeling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dlab",
   "language": "python",
   "name": "dlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
