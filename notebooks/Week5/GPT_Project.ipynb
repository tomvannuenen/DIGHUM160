{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c7cfced-157b-4977-b7d6-bf274f3008cc",
   "metadata": {},
   "source": [
    "# GPT Fintuning - PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197ab1a7-32fb-4484-8cae-f076bdaeac4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "print(torch.__version__)\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1736b828-554a-418d-8c25-82724d2d4112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Set the seed for PyTorch (controls randomness for reproducibility)\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2')\n",
    "\n",
    "# Encode input context to get token IDs\n",
    "input_text = \"PASTE A TITLE OF A POST FROM YOUR SUBREDDIT\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# Generate text using the model\n",
    "output = model.generate(input_ids, \n",
    "                        do_sample=True, \n",
    "                        max_length=150, \n",
    "                        repetition_penalty=1.1,\n",
    "                        temperature=.5, \n",
    "                        top_k=30, \n",
    "                        top_p=0.95\n",
    "                        )\n",
    "\n",
    "# Decode the generated IDs to text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe29ece-48d6-43a4-a910-171462e2ae4c",
   "metadata": {},
   "source": [
    "üîî **Question**: Does this output make sense? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4187184a-f7c6-4420-8565-75a1af040f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2')\n",
    "embeddings = model.transformer.wte.weight.detach().numpy()\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "# add as many words as needed\n",
    "words = [\"word1\", \"word2\", \"word3\", \"word4\"]\n",
    "word_indices = [tokenizer.encode(word)[0] for word in words]\n",
    "selected_embeddings = embeddings[word_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f8f39b-2f53-4967-827c-5321c84e7935",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988de898-906f-4074-a8e4-a122eb424986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Using PCA\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings_pca = pca.fit_transform(selected_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9aea48-e363-4da1-81a8-21671f98fc5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "\n",
    "# Function to plot\n",
    "def plot_embeddings(embeddings, labels, title='PCA of GPT-2 Embeddings'):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    texts = []\n",
    "    points = plt.scatter(embeddings[:, 0], embeddings[:, 1])  # scatter plot of embeddings\n",
    "\n",
    "    # Generate text annotations\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = embeddings[i]\n",
    "        text = plt.text(x, y, label, ha='right', va='bottom', fontsize=9)\n",
    "        texts.append(text)\n",
    "\n",
    "    # Use adjust_text to avoid overlapping\n",
    "    adjust_text(texts, x=embeddings[:, 0], y=embeddings[:, 1], arrowprops=dict(arrowstyle='->', color='red', lw=0.5))\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Save plot\n",
    "plt.savefig('gpt2_embeddings_pca.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Example usage (assuming 'reduced_embeddings_pca' and 'words' are defined)\n",
    "plot_embeddings(reduced_embeddings_pca, words, title='PCA of GPT-2 Embeddings')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035ea2ae-a86e-480f-b24b-45ebf3e6c645",
   "metadata": {},
   "source": [
    "<a id=\"ft\"></a>\n",
    "\n",
    "# Finetuning GPT-2\n",
    "\n",
    "### ‚ö†Ô∏è Warning\n",
    "Even though we are only training a small model, the following blocks of code operation will take long on a consumer-grade PC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e276c0-175c-4705-8726-88fea42ba44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('../../data/YOUR_DATA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83a48a9-00a9-47d6-aa12-57589f6a9e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['selftext'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef2def2-af4f-457b-b1a6-bfd1a6b7d700",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dee921-ed31-42e2-b567-03c57af0259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "\n",
    "# Initialize tokenizer with padding token set\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize texts\n",
    "encodings = tokenizer(texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985b2ea0-4a01-4eef-bef7-0167f685c0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        # For language modeling, the labels are the input_ids shifted by one\n",
    "        item[\"labels\"] = item[\"input_ids\"].clone()\n",
    "        return item\n",
    "\n",
    "# Initialize the dataset\n",
    "train_dataset = TextDataset(encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7190723-9384-4846-be8e-5c6707daca84",
   "metadata": {},
   "source": [
    "## Commence Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d495b137-64a2-49c3-ab30-c891e544b37f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2')\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='../../results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    logging_dir='../../logs'\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6119bf70-2b1d-41ed-8e6e-f160021b2845",
   "metadata": {},
   "source": [
    "If you did run the previous code, make sure to save the model and finetuned tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaf0410-eeef-4158-92c2-cc5a5012ed6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = 'my_ft_model'\n",
    "tokenizer_save_path = 'my_ft_tokenizer'\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(model_save_path)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(tokenizer_save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ffb4de-a32e-4881-b8bc-98ed53272665",
   "metadata": {},
   "source": [
    "<a id=\"int\"></a>\n",
    "# Interpreting Model Output\n",
    "\n",
    "Let's have a look at the ways DistilGPT2's behavior has been altered due to the finetuning on r/aita.\n",
    "\n",
    "I uploaded my finetuned model to [HuggingFace](https://huggingface.co/tvannuenen/finetuned_model) so we can download it from there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa91aaf-3c9b-4f1c-b212-da675a2ac422",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load the tokenizer and model from the Hugging Face Hub\n",
    "ft_tokenizer = GPT2Tokenizer.from_pretrained('my_ft_tokenizer')\n",
    "ft_model = GPT2LMHeadModel.from_pretrained('my_ft_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017b05b8-b40b-4c32-a819-b190927154e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Set the seed for PyTorch (controls randomness for reproducibility)\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "def generate_text(model, prompt, do_sample=True, max_length=50, temperature=1, top_k=50, top_p=0.95, repetition_penalty=1.1):\n",
    "    \"\"\"\n",
    "    Generates text based on a given prompt using the specified model.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The fine-tuned model to use for text generation.\n",
    "    - prompt: The initial text to start generating from.\n",
    "    - max_length: Maximum length of the generated text.\n",
    "    - temperature: Sampling temperature for generating text.\n",
    "    - top_k: The number of highest probability vocabulary tokens to keep for top-k filtering.\n",
    "    - top_p: Nucleus sampling's cumulative probability cutoff to keep for top-p filtering.\n",
    "    \n",
    "    Returns:\n",
    "    - generated_text: The generated text as a string.\n",
    "    \"\"\"\n",
    "    # Encode the prompt text to tensor\n",
    "    input_ids = ft_tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    # Generate a sequence of tokens following the prompt\n",
    "    output_ids = ft_model.generate(input_ids, max_length=max_length, \n",
    "                                temperature=temperature, \n",
    "                                do_sample=do_sample, \n",
    "                                top_k=top_k, \n",
    "                                top_p=top_p, \n",
    "                                repetition_penalty=repetition_penalty, \n",
    "                                pad_token_id=ft_tokenizer.eos_token_id)\n",
    "    \n",
    "    # Decode the generated tokens to a string\n",
    "    generated_text = ft_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Prompt to generate text from - play around with this!\n",
    "prompt = \"PASTE A TITLE FROM YOUR DATA\"\n",
    "\n",
    "# Generate texts\n",
    "generated_text = generate_text(ft_model, prompt, max_length=150)\n",
    "print(\"Generated text from finetuned model:\", generated_text, '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fd4cad-a9ef-454d-987d-97d3f98fecc3",
   "metadata": {},
   "source": [
    "## Visualizing the Finetuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c800f3-fb0c-402d-8335-66f9cb816d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = ft_model.transformer.wte.weight.detach().numpy()\n",
    "\n",
    "# Use the same words you did on the pretrained model\n",
    "words = [\"word1\", \"word2\", \"word3\", \"word4\", \"word5\"]\n",
    "word_indices = [ft_tokenizer.encode(word)[0] for word in words]\n",
    "selected_embeddings = embeddings[word_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3071da83-c2b0-4b3a-9911-04fad0730e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Using PCA\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings_pca = pca.fit_transform(selected_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d835c0-a122-4611-9ee3-0c23d394f326",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "\n",
    "# Function to plot\n",
    "def plot_embeddings(embeddings, labels, title='PCA of GPT-2 Embeddings'):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    texts = []\n",
    "    points = plt.scatter(embeddings[:, 0], embeddings[:, 1])  # scatter plot of embeddings\n",
    "\n",
    "    # Generate text annotations\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = embeddings[i]\n",
    "        text = plt.text(x, y, label, ha='right', va='bottom', fontsize=9)\n",
    "        texts.append(text)\n",
    "\n",
    "    # Use adjust_text to avoid overlapping\n",
    "    adjust_text(texts, x=embeddings[:, 0], y=embeddings[:, 1], arrowprops=dict(arrowstyle='->', color='red', lw=0.5))\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.savefig('finetuned_gpt2_embeddings_pca.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage (assuming 'reduced_embeddings_pca' and 'words' are defined)\n",
    "plot_embeddings(reduced_embeddings_pca, words, title='PCA of GPT-2 Embeddings')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287e8d67-c034-440f-842d-86b6ce545314",
   "metadata": {},
   "source": [
    "# Create Posts Using Finetuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44aa533-bb8b-4577-9e8c-0f300f3b0abb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, pipeline\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('../../data/YOUR_DATA.csv')\n",
    "\n",
    "# Randomly select 10 entries\n",
    "sample_df = df.sample(n=10, random_state=1)  # Use a fixed seed for reproducibility, if needed\n",
    "\n",
    "original_texts = sample_df['selftext'].tolist()  # Adjust column name if different\n",
    "titles = sample_df['title'].tolist()\n",
    "\n",
    "# Initialize the generation pipeline\n",
    "generator = pipeline('text-generation', model=ft_model, tokenizer=ft_tokenizer, device=-1)  # CPU usage\n",
    "\n",
    "# Prepare to generate texts\n",
    "generated_texts = []\n",
    "for title, original_text in zip(titles, original_texts):\n",
    "    # Calculate the length of the original post in tokens\n",
    "    target_length = len(ft_tokenizer.encode(original_text))\n",
    "\n",
    "    # Generate a new post of the same length starting from the title\n",
    "    # Ensure to set max_length to the length of the original post\n",
    "    prompt = title\n",
    "    generated = generator(prompt, max_length=target_length, num_return_sequences=1)[0]['generated_text']\n",
    "    generated_texts.append(generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlab",
   "language": "python",
   "name": "dlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
