{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c7cfced-157b-4977-b7d6-bf274f3008cc",
   "metadata": {},
   "source": [
    "# GPT Fintuning - PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be00427a-62a9-49fa-8ee6-cecf08d68dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following to install PyTorch and Transformers libraries\n",
    "\n",
    "#%pip install torch\n",
    "#%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642a7038-81fc-4ed4-9792-c0365f88eb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "print(torch.__version__)\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f5627c-b645-4505-9347-f5a45842724d",
   "metadata": {},
   "source": [
    "Reviewing a pretrained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00216452-6fd2-48e8-a6fc-e77a96e5fc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Set the seed for PyTorch (controls randomness for reproducibility)\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2')\n",
    "\n",
    "# Encode input context to get token IDs\n",
    "input_text = \"AITA for pretending to get fired when customers get a temper with me?\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# Generate text using the model\n",
    "output = model.generate(input_ids, \n",
    "                        do_sample=True, \n",
    "                        max_length=150, \n",
    "                        repetition_penalty=1.1,\n",
    "                        temperature=1, \n",
    "                        top_k=50, \n",
    "                        top_p=0.95\n",
    "                        )\n",
    "\n",
    "# Decode the generated IDs to text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267d9ceb-31d3-49a3-99fe-63708cc6612b",
   "metadata": {},
   "source": [
    "üîî **Question**: Does this output make sense? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167c9dd8-87bb-4960-b3b8-2487aaa257fd",
   "metadata": {},
   "source": [
    "### Visualizing Embeddings \n",
    "\n",
    "Change the words in the `words` variable to words you are interested in exploring. \n",
    "\n",
    "Think about your own data already: which words  might show up often in the Reddit data you are working with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06834ec6-9d29-44cb-bb8a-2b700e3d7e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2')\n",
    "embeddings = model.transformer.wte.weight.detach().numpy()\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "words = [\"mother\", \"father\", \"wife\", \"husband\"]\n",
    "word_indices = [tokenizer.encode(word)[0] for word in words]\n",
    "selected_embeddings = embeddings[word_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace20470-c867-4b07-ad2b-8e81eb895074",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa3c3b8-ab5c-4038-b06e-b5aabbba0395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Using PCA\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings_pca = pca.fit_transform(selected_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256f18ee-2d96-4fb9-afaa-3a7b96b544e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Uncomment the following line to install adjustText \n",
    "\n",
    "#%pip install adjustText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c25d86-ba20-483d-a8b6-41300c9bba37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "\n",
    "# Function to plot\n",
    "def plot_embeddings(embeddings, labels, title='PCA of GPT-2 Embeddings'):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    texts = []\n",
    "    points = plt.scatter(embeddings[:, 0], embeddings[:, 1])  # scatter plot of embeddings\n",
    "\n",
    "    # Generate text annotations\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = embeddings[i]\n",
    "        text = plt.text(x, y, label, ha='right', va='bottom', fontsize=9)\n",
    "        texts.append(text)\n",
    "\n",
    "    # Use adjust_text to avoid overlapping\n",
    "    adjust_text(texts, x=embeddings[:, 0], y=embeddings[:, 1], arrowprops=dict(arrowstyle='->', color='red', lw=0.5))\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage (assuming 'reduced_embeddings_pca' and 'words' are defined)\n",
    "plot_embeddings(reduced_embeddings_pca, words, title='PCA of GPT-2 Embeddings')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becc72ec-a07e-4a86-a3d7-649fb706d0c5",
   "metadata": {},
   "source": [
    "<a id=\"ft\"></a>\n",
    "\n",
    "# Finetuning GPT-2 \n",
    "\n",
    "In all of the cells below, replace YOUR_FILE with the name of the file you are working with.\n",
    "\n",
    "### ‚ö†Ô∏è Warning\n",
    "Even though we are only training a small model, the following blocks of code operation will take long on a consumer-grade PC (for reference: around 4 hours on an Apple M2 Pro with 16GB memory). I have run this code and saved the model, so you don't have to run it yourself. Please move on to \"Getting the Model From Google Drive\". \n",
    "\n",
    "If you do choose to run the model yourself, you may want to run this notebook in Google Colab using a GPU. See bCourses for a link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6891802e-b0f1-4050-ab95-cafcd43c18ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('../../data/YOUR_FILE.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaa3aaa-1268-4246-b5bd-dd040c7f13d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['selftext'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445ca2cc-ee7c-4c26-a888-0a292434ec40",
   "metadata": {},
   "source": [
    "## Commence Tokenization\n",
    "\n",
    "We will tokenize the entire texts with truncation and padding to a fixed maximum length. This method is straightforward and treats each text as an individual sequence for the model to learn from. The main characteristics include:\n",
    "\n",
    "- `truncation`: Texts longer than `max_length=512` are cut off, potentially losing important information at the end.\n",
    "- `padding`: Texts shorter than `max_length=512` are padded to ensure uniform sequence length, usually with the pad_token. This is not relevant to us as all texts we are feeding into the model are much longer than 512 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644bfca0-7def-4af7-9734-5d3e46048413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "\n",
    "# Initialize tokenizer with padding token set\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize texts\n",
    "encodings = tokenizer(texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a14f61-3ae6-4d64-b67f-40ee6015bde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        # For language modeling, the labels are the input_ids shifted by one\n",
    "        item[\"labels\"] = item[\"input_ids\"].clone()\n",
    "        return item\n",
    "\n",
    "# Initialize the dataset\n",
    "train_dataset = TextDataset(encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ec4274-17b5-4a3a-89ab-ab7243820c53",
   "metadata": {},
   "source": [
    "## Commence Finetuning\n",
    "\n",
    "In the following cell, we initialize the fine-tuning process using Hugging Face's `Trainer` class.\n",
    "\n",
    "‚ö†Ô∏è **Warning:** Even though we are using a small model, the following operation will take *long* on a consumer-grade PC (for reference: around 4 hours on an Apple M2 Pro with 16GB memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32e74a3-1e2e-45f2-95e4-391f4034ac7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2')\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='../../results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    logging_dir='../../logs'\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45da43c6-536d-42df-8698-62a5a4af7749",
   "metadata": {},
   "source": [
    "If you did run the previous code, make sure to save the model and finetuned tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8368f6-c45c-4f6a-805b-b010a8c7e8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = '../../models/my_finetuned_model'\n",
    "tokenizer_save_path = '../../models/my_finetuned_tokenizer'\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(model_save_path)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(tokenizer_save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63102fee-7a30-44db-956d-66f7fca13724",
   "metadata": {},
   "source": [
    "<a id=\"int\"></a>\n",
    "# Interpreting Model Output\n",
    "\n",
    "Enter your own prompt using the `prompt` variable. Pick a prompt that resembles posts in your subreddit (e.g. a title, or the first few words of a post)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73eb5f3-527a-45f0-8802-a5b95181ca65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Set the seed for PyTorch (controls randomness for reproducibility)\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "def generate_text(model, prompt, do_sample=True, max_length=50, temperature=1, top_k=50, top_p=0.95, repetition_penalty=1.1):\n",
    "    \"\"\"\n",
    "    Generates text based on a given prompt using the specified model.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The fine-tuned model to use for text generation.\n",
    "    - prompt: The initial text to start generating from.\n",
    "    - max_length: Maximum length of the generated text.\n",
    "    - temperature: Sampling temperature for generating text.\n",
    "    - top_k: The number of highest probability vocabulary tokens to keep for top-k filtering.\n",
    "    - top_p: Nucleus sampling's cumulative probability cutoff to keep for top-p filtering.\n",
    "    \n",
    "    Returns:\n",
    "    - generated_text: The generated text as a string.\n",
    "    \"\"\"\n",
    "    # Encode the prompt text to tensor\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    # Generate a sequence of tokens following the prompt\n",
    "    output_ids = model.generate(input_ids, max_length=max_length, \n",
    "                                temperature=temperature, \n",
    "                                do_sample=do_sample, \n",
    "                                top_k=top_k, \n",
    "                                top_p=top_p, \n",
    "                                repetition_penalty=repetition_penalty, \n",
    "                                pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    # Decode the generated tokens to a string\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Load the fine-tuned model\n",
    "ft_model = GPT2LMHeadModel.from_pretrained('../../models/my_finetuned_model')\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('../../models/my_finetuned_tokenizer')\n",
    "\n",
    "# Prompt to generate text from - play around with this!\n",
    "prompt = \"ENTER YOUR OWN PROMPT HERE\"\n",
    "\n",
    "# Generate texts\n",
    "generated_text = generate_text(ft_model, prompt, max_length=150)\n",
    "print(\"Generated text from finetuned model:\", generated_text, '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fbfb58-1bd8-4fa1-839d-418154b6db3d",
   "metadata": {},
   "source": [
    "## Visualizing Finetuned Embeddings\n",
    "\n",
    "Let's look at the embedding space.\n",
    "\n",
    "Change the words in the `words` variable to words you are interested in exploring. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4898cee5-b42e-4b3a-bf82-dc16ed419d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = GPT2LMHeadModel.from_pretrained('../../models/my_finetuned_model')\n",
    "embeddings = ft_model.transformer.wte.weight.detach().numpy()\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('../../models/my_finetuned_tokenizer')\n",
    "words = [\"mother\", \"father\", \"wife\", \"husband\"]\n",
    "word_indices = [tokenizer.encode(word)[0] for word in words]\n",
    "selected_embeddings = embeddings[word_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ec9f4c-60f6-420b-8e21-d1a1e413ef17",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188252d0-8c5b-4394-a6e4-ed868938851d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Using PCA\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings_pca = pca.fit_transform(selected_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36edfbf-0866-48bf-af07-6d15b3ec7e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "\n",
    "# Function to plot\n",
    "def plot_embeddings(embeddings, labels, title='PCA of GPT-2 Embeddings'):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    texts = []\n",
    "    points = plt.scatter(embeddings[:, 0], embeddings[:, 1])  # scatter plot of embeddings\n",
    "\n",
    "    # Generate text annotations\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = embeddings[i]\n",
    "        text = plt.text(x, y, label, ha='right', va='bottom', fontsize=9)\n",
    "        texts.append(text)\n",
    "\n",
    "    # Use adjust_text to avoid overlapping\n",
    "    adjust_text(texts, x=embeddings[:, 0], y=embeddings[:, 1], arrowprops=dict(arrowstyle='->', color='red', lw=0.5))\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage (assuming 'reduced_embeddings_pca' and 'words' are defined)\n",
    "plot_embeddings(reduced_embeddings_pca, words, title='PCA of Finetuned GPT-2 Embeddings')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2dfad6-93a0-452c-ab00-46681e5cb0e5",
   "metadata": {},
   "source": [
    "# Create Posts Using Finetuned Model\n",
    "\n",
    "We will ask the model to generate text based on the title of the original posts in your data. We will also ask it to create a post that is the same length as the original post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233fcbf4-8845-4327-9ddf-5280d862ca3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, pipeline\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('../../data/YOUR_FILE.csv')\n",
    "\n",
    "# Randomly select 10 entries\n",
    "sample_df = df.sample(n=10, random_state=1)  # Use a fixed seed for reproducibility, if needed\n",
    "\n",
    "original_texts = sample_df['selftext'].tolist()  # Adjust column name if different\n",
    "titles = sample_df['title'].tolist()\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('../../models/my_finetuned_tokenizer')\n",
    "model = GPT2LMHeadModel.from_pretrained('../../models/my_finetuned_model')\n",
    "\n",
    "# Initialize the generation pipeline\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=-1)  # CPU usage\n",
    "\n",
    "# Prepare to generate texts\n",
    "generated_texts = []\n",
    "for title, original_text in zip(titles, original_texts):\n",
    "    # Calculate the length of the original post in tokens\n",
    "    target_length = len(tokenizer.encode(original_text))\n",
    "\n",
    "    # Generate a new post of the same length starting from the title\n",
    "    # Ensure to set max_length to the length of the original post\n",
    "    prompt = title\n",
    "    generated = generator(prompt, max_length=target_length, num_return_sequences=1)[0]['generated_text']\n",
    "    generated_texts.append(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9418715-da75-4cb7-9fe3-eb5e124c4637",
   "metadata": {},
   "source": [
    "Let's save the original and generated posts in a new DataFrame so we can easily compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515b75c2-4026-4d19-afeb-c0530bd63fe3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'original_text': original_texts,\n",
    "    'generated_text': generated_texts,\n",
    "    'title': titles  # Assuming you have a list of titles\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43698dcd-e182-4a94-9ecf-32fffabdc6b5",
   "metadata": {},
   "source": [
    "Let's compare original posts to the ones we created with GPT-2. Here we are selecting an index from our new DataFrame and printing the text for both the original and our generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b4c479-2e30-4c4d-bf8c-3dcd0b3e8d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 2\n",
    "print(f\"Title: {df.loc[index]['title']}\")\n",
    "print(\"\\n\")\n",
    "print(f\"Original Text: {df.loc[index, 'original_text']}\")\n",
    "print(\"\\n\")\n",
    "print(f\"Generated Text: {df.loc[index, 'generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c2ced7-e06d-435e-9f59-1c9cbdef7f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cea5ec-76a9-48cb-a2c9-e92245d63a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def compare_posts(index):\n",
    "    if index < 0 or index >= len(df):\n",
    "        print(\"Invalid index, please choose a valid post index.\")\n",
    "        return\n",
    "    \n",
    "    # Display the texts\n",
    "    print(f\"Title: {df.loc[index, 'title']}\")\n",
    "\n",
    "    # Sentiment analysis\n",
    "    original_blob = TextBlob(df.loc[index, 'original_text'])\n",
    "    generated_blob = TextBlob(df.loc[index, 'generated_text'])\n",
    "    print(\"\\nSentiment Analysis:\")\n",
    "    print(f\"Original Sentiment: Polarity = {original_blob.sentiment.polarity}, Subjectivity = {original_blob.sentiment.subjectivity}\")\n",
    "    print(f\"Generated Sentiment: Polarity = {generated_blob.sentiment.polarity}, Subjectivity = {generated_blob.sentiment.subjectivity}\")\n",
    "    \n",
    "# Example usage: compare post at index 5\n",
    "compare_posts(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdc5936-01da-4ba0-aea5-53cb4b1531f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a function to calculate sentiment polarity\n",
    "def get_sentiment_polarity(texts):\n",
    "    return [TextBlob(text).sentiment.polarity for text in texts]\n",
    "\n",
    "# Calculate sentiment polarity for both original and generated texts\n",
    "df['original_polarity'] = get_sentiment_polarity(df['original_text'])\n",
    "df['generated_polarity'] = get_sentiment_polarity(df['generated_text'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['original_polarity'], df['generated_polarity'], alpha=0.5, color='blue')\n",
    "plt.title('Comparison of Sentiment Polarity between Original and Generated Texts')\n",
    "plt.xlabel('Original Text Sentiment Polarity')\n",
    "plt.ylabel('Generated Text Sentiment Polarity')\n",
    "plt.grid(True)\n",
    "plt.axhline(y=0, color='gray', linestyle='--')\n",
    "plt.axvline(x=0, color='gray', linestyle='--')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee8e0a7-3ce1-4dae-a26a-f3aa8c4c2ce5",
   "metadata": {},
   "source": [
    "## Back to TF-IDF\n",
    "\n",
    "Use the TF-IDF algorithm to check the similarity between original texts and the ones we generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9533349-ed85-4f5c-a275-6601895f78b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Combine original and generated texts into one list for TF-IDF analysis\n",
    "texts = df['original_text'].tolist() + df['generated_text'].tolist()\n",
    "\n",
    "# Initialize a TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the data\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Calculate cosine similarity between original and generated texts\n",
    "# Assuming the first half are originals and the second half are generated\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix[:len(df)], tfidf_matrix[len(df):])\n",
    "\n",
    "# Display similarity results\n",
    "for i, similarity in enumerate(similarity_matrix.diagonal()):\n",
    "    print(f\"Text {i+1} Similarity between original and generated: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58567cd8-0240-4281-bafa-0c7559d7bae7",
   "metadata": {},
   "source": [
    "## Back to Word Embeddings\n",
    "\n",
    "Project the reduced embeddings for both the original and our generated posts in 2D space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f7ee4b-e655-41a7-b1e3-5d3cab839f76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load the medium model with word vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample data (assuming df is already loaded)\n",
    "original_texts = df['original_text'].tolist()[:10]  # Limiting for simplicity\n",
    "generated_texts = df['generated_text'].tolist()[:10]\n",
    "\n",
    "# Function to compute average embeddings for a text\n",
    "def get_average_embedding(text, nlp_model):\n",
    "    doc = nlp_model(text)\n",
    "    vectors = [word.vector for word in doc if not word.is_stop and word.has_vector]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros((nlp_model.vocab.vectors_length,))\n",
    "\n",
    "# Compute average embeddings for each set of texts\n",
    "original_embeddings = np.array([get_average_embedding(text, nlp) for text in original_texts])\n",
    "generated_embeddings = np.array([get_average_embedding(text, nlp) for text in generated_texts])\n",
    "\n",
    "# Perform PCA to reduce dimensionality for visualization\n",
    "pca = PCA(n_components=2)\n",
    "original_pca = pca.fit_transform(original_embeddings)\n",
    "generated_pca = pca.transform(generated_embeddings)\n",
    "\n",
    "# Plotting the embeddings\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(original_pca[:, 0], original_pca[:, 1], color='blue', label='Original', alpha=0.5)\n",
    "plt.scatter(generated_pca[:, 0], generated_pca[:, 1], color='red', label='Generated', alpha=0.5)\n",
    "plt.title('Comparison of Word Embeddings')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlab",
   "language": "python",
   "name": "dlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
