{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c7cfced-157b-4977-b7d6-bf274f3008cc",
   "metadata": {},
   "source": [
    "# GPT Fintuning - PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79072b13-edfd-4e82-8bbd-23d5eb383732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "print(torch.__version__)\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc83e28-2a95-43f0-a9fd-c32c61232a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Set the seed for PyTorch (controls randomness for reproducibility)\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2')\n",
    "\n",
    "# Encode input context to get token IDs\n",
    "input_text = \"PASTE TITLE FROM YOUR SUBREDDIT\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# Generate text using the model\n",
    "output = model.generate(input_ids, \n",
    "                        do_sample=True, \n",
    "                        max_length=150, \n",
    "                        repetition_penalty=1.1,\n",
    "                        temperature=.5, \n",
    "                        top_k=30, \n",
    "                        top_p=0.95\n",
    "                        )\n",
    "\n",
    "# Decode the generated IDs to text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c8a5c4-e863-4032-ab22-03d0c79ff068",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2')\n",
    "embeddings = model.transformer.wte.weight.detach().numpy()\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "words = [\"term1\", \"term2\", \"term3\", \"term4\", \"term5\"]\n",
    "word_indices = [tokenizer.encode(word)[0] for word in words]\n",
    "selected_embeddings = embeddings[word_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31856071-fec0-4f66-a3bb-021dd89699b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7f443d-d725-4b2e-aae3-2261ffe97062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Using PCA\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings_pca = pca.fit_transform(selected_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a417649f-a325-43e5-80eb-41760369d158",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "\n",
    "# Function to plot\n",
    "def plot_embeddings(embeddings, labels, title='PCA of GPT-2 Embeddings'):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    texts = []\n",
    "    points = plt.scatter(embeddings[:, 0], embeddings[:, 1])  # scatter plot of embeddings\n",
    "\n",
    "    # Generate text annotations\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = embeddings[i]\n",
    "        text = plt.text(x, y, label, ha='right', va='bottom', fontsize=9)\n",
    "        texts.append(text)\n",
    "\n",
    "    # Use adjust_text to avoid overlapping\n",
    "    adjust_text(texts, x=embeddings[:, 0], y=embeddings[:, 1], arrowprops=dict(arrowstyle='->', color='red', lw=0.5))\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Save plot\n",
    "plt.savefig('outputs_project/gpt2_embeddings_pca.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Example usage (assuming 'reduced_embeddings_pca' and 'words' are defined)\n",
    "plot_embeddings(reduced_embeddings_pca, words, title='PCA of GPT-2 Embeddings')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f4397d-e216-45b0-9326-f38d4bc475fd",
   "metadata": {},
   "source": [
    "<a id=\"ft\"></a>\n",
    "\n",
    "# Finetuning GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1d4e5f-3557-481e-8833-31961ae0eac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('../../data/YOUR_DATA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e211253-031e-4c22-8e4f-a4375adf4695",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['selftext'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2562fee0-7299-415e-ae8e-2ef6dbd34946",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9acc0a1-a66b-4c4e-9cb9-5ee25d21651b",
   "metadata": {},
   "source": [
    "## Commence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f36166-5276-46c1-bdf5-80bc66bb7a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "\n",
    "# Initialize tokenizer with padding token set\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize texts\n",
    "encodings = tokenizer(texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39930bd-60ac-4ccf-bcc5-c098db586e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        # For language modeling, the labels are the input_ids shifted by one\n",
    "        item[\"labels\"] = item[\"input_ids\"].clone()\n",
    "        return item\n",
    "\n",
    "# Initialize the dataset\n",
    "train_dataset = TextDataset(encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e59234-78a6-42c6-849b-981643b44334",
   "metadata": {},
   "source": [
    "## Commence Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9e14f1-dcc7-4555-8521-42f3520943ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2')\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='../../results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    logging_dir='../../logs'\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d625bd-783a-4860-ae51-d92c62917bf8",
   "metadata": {},
   "source": [
    "If you did run the previous code, make sure to save the model and finetuned tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84e2a0f-c878-4c62-95de-3f83fe53db9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = 'outputs_project/ft_model'\n",
    "tokenizer_save_path = 'outputs_project/ft_tokenizer'\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(model_save_path)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(tokenizer_save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6631a3d1-f6f8-4b00-a635-5aeea53d6d12",
   "metadata": {},
   "source": [
    "<a id=\"int\"></a>\n",
    "# Interpreting Model Output\n",
    "\n",
    "Let's have a look at the ways DistilGPT2's behavior has been altered due to the finetuning on r/aita.\n",
    "\n",
    "I uploaded my finetuned model to [HuggingFace](https://huggingface.co/tvannuenen/finetuned_model) so we can download it from there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e094f65e-cfba-483d-8d7a-eab5bbc5f20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load the tokenizer and model from the Hugging Face Hub\n",
    "ft_tokenizer = GPT2Tokenizer.from_pretrained('outputs_project/ft_tokenizer')\n",
    "ft_model = GPT2LMHeadModel.from_pretrained('outputs_project/ft_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663b17e8-3c00-46dd-80d9-3ec51cdd4bff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Set the seed for PyTorch (controls randomness for reproducibility)\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "def generate_text(model, prompt, do_sample=True, max_length=50, temperature=1, top_k=50, top_p=0.95, repetition_penalty=1.1):\n",
    "    \"\"\"\n",
    "    Generates text based on a given prompt using the specified model.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The fine-tuned model to use for text generation.\n",
    "    - prompt: The initial text to start generating from.\n",
    "    - max_length: Maximum length of the generated text.\n",
    "    - temperature: Sampling temperature for generating text.\n",
    "    - top_k: The number of highest probability vocabulary tokens to keep for top-k filtering.\n",
    "    - top_p: Nucleus sampling's cumulative probability cutoff to keep for top-p filtering.\n",
    "    \n",
    "    Returns:\n",
    "    - generated_text: The generated text as a string.\n",
    "    \"\"\"\n",
    "    # Encode the prompt text to tensor\n",
    "    input_ids = ft_tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    # Generate a sequence of tokens following the prompt\n",
    "    output_ids = ft_model.generate(input_ids, max_length=max_length, \n",
    "                                temperature=temperature, \n",
    "                                do_sample=do_sample, \n",
    "                                top_k=top_k, \n",
    "                                top_p=top_p, \n",
    "                                repetition_penalty=repetition_penalty, \n",
    "                                pad_token_id=ft_tokenizer.eos_token_id)\n",
    "    \n",
    "    # Decode the generated tokens to a string\n",
    "    generated_text = ft_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Prompt to generate text from - play around with this!\n",
    "prompt = \"PASTE TITLE FROM YOUR SUBREDDIT\"\n",
    "\n",
    "# Generate texts\n",
    "generated_text = generate_text(ft_model, prompt, max_length=150)\n",
    "print(\"Generated text from finetuned model:\", generated_text, '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45f9822-597b-4aea-a613-d94fb8b22cfc",
   "metadata": {},
   "source": [
    "## Visualizing the Finetuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beea945e-e73f-4a6c-95bb-710d3057c412",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = ft_model.transformer.wte.weight.detach().numpy()\n",
    "\n",
    "words = [\"term1\", \"term2\", \"term3\", \"term4\", \"term5\"]\n",
    "word_indices = [ft_tokenizer.encode(word)[0] for word in words]\n",
    "selected_embeddings = embeddings[word_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c625488-7d16-4aff-84a2-05c1217ff773",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Using PCA\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings_pca = pca.fit_transform(selected_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97ec6bd-696b-4d5a-bace-b62d546884fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "\n",
    "# Function to plot\n",
    "def plot_embeddings(embeddings, labels, title='PCA of GPT-2 Embeddings'):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    texts = []\n",
    "    points = plt.scatter(embeddings[:, 0], embeddings[:, 1])  # scatter plot of embeddings\n",
    "\n",
    "    # Generate text annotations\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = embeddings[i]\n",
    "        text = plt.text(x, y, label, ha='right', va='bottom', fontsize=9)\n",
    "        texts.append(text)\n",
    "\n",
    "    # Use adjust_text to avoid overlapping\n",
    "    adjust_text(texts, x=embeddings[:, 0], y=embeddings[:, 1], arrowprops=dict(arrowstyle='->', color='red', lw=0.5))\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.savefig('outputs_project/finetuned_gpt2_embeddings_pca.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage (assuming 'reduced_embeddings_pca' and 'words' are defined)\n",
    "plot_embeddings(reduced_embeddings_pca, words, title='PCA of GPT-2 Embeddings')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f5dc57-1da0-4624-bc16-fffe5678cd27",
   "metadata": {},
   "source": [
    "# Create Posts Using Finetuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b045770-43cb-48aa-b7dd-79f8eec9c731",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, pipeline\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('../../data/aita_pp.csv')\n",
    "\n",
    "# Randomly select 10 entries\n",
    "sample_df = sample_df.sample(n=10, random_state=1).reset_index()  # Keep original index as a column\n",
    "\n",
    "original_texts = [f\"{t}\\n\\n{s}\" for t, s in zip(sample_df['title'], sample_df['selftext'])]\n",
    "titles = sample_df['title'].tolist()\n",
    "\n",
    "# Initialize the generation pipeline\n",
    "generator = pipeline('text-generation', model=ft_model, tokenizer=ft_tokenizer, device=-1)  # CPU usage\n",
    "\n",
    "# Prepare to generate texts\n",
    "generated_texts = []\n",
    "for title, original_text in zip(titles, original_texts):\n",
    "    # Calculate the length of the original post in tokens\n",
    "    target_length = len(ft_tokenizer.encode(original_text))\n",
    "\n",
    "    # Generate a new post of the same length starting from the title\n",
    "    # Ensure to set max_length to the length of the original post\n",
    "    prompt = title\n",
    "    generated = generator(prompt, max_length=target_length, num_return_sequences=1)[0]['generated_text']\n",
    "    generated_texts.append(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3745f073-1366-44ee-9526-6e7deb33a64a",
   "metadata": {},
   "source": [
    "Let's save the original and generated posts in a new DataFrame so we can easily compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6100c74e-07fd-4c79-8992-4e0175004b73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'original_text': original_texts,\n",
    "    'generated_text': generated_texts,\n",
    "    'title': titles  # Assuming you have a list of titles\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df742ba9-8891-4ea2-bb35-22fb09fd3751",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4feaeb1-9d62-4472-b93d-796c478e3b25",
   "metadata": {},
   "source": [
    "## Back to TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b280a2ee-a47d-4fa9-97b2-ff43008064b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Combine original and generated texts into one list for TF-IDF analysis\n",
    "texts = df['original_text'].tolist() + df['generated_text'].tolist()\n",
    "\n",
    "# Initialize a TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the data\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Calculate cosine similarity between original and generated texts\n",
    "# Assuming the first half are originals and the second half are generated\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix[:len(df)], tfidf_matrix[len(df):])\n",
    "\n",
    "# Display similarity results\n",
    "for i, similarity in enumerate(similarity_matrix.diagonal()):\n",
    "    print(f\"Text {i+1} Similarity between original and generated: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25143f4c-534c-412a-b363-3a1bdacf27c7",
   "metadata": {},
   "source": [
    "## Back to Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e732689-240c-48a3-b96c-8b5d7f849545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load the medium model with word vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample data\n",
    "original_texts = [f\"[Post {i}] {title}\\n\\n{body}\" \n",
    "                  for i, title, body in zip(sample_df['index'], sample_df['title'], sample_df['selftext'])]\n",
    "generated_texts = [f\"[Post {i}] {gen}\" \n",
    "                   for i, gen in zip(sample_df['index'], generated_texts)]\n",
    "\n",
    "# Function to compute average embeddings for a text\n",
    "def get_average_embedding(text, nlp_model):\n",
    "    doc = nlp_model(text)\n",
    "    vectors = [word.vector for word in doc if not word.is_stop and word.has_vector]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros((nlp_model.vocab.vectors_length,))\n",
    "\n",
    "# Compute average embeddings for each set of texts\n",
    "original_embeddings = np.array([get_average_embedding(text, nlp) for text in original_texts])\n",
    "generated_embeddings = np.array([get_average_embedding(text, nlp) for text in generated_texts])\n",
    "\n",
    "# Perform PCA to reduce dimensionality for visualization\n",
    "pca = PCA(n_components=2)\n",
    "original_pca = pca.fit_transform(original_embeddings)\n",
    "generated_pca = pca.transform(generated_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4469aba3-d4b7-45d9-bd78-60b6f4000077",
   "metadata": {},
   "source": [
    "Plot it with `bokeh`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f581b4b7-f807-4b8f-be7f-e4ebe962a0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "from bokeh.io import output_file\n",
    "import numpy as np\n",
    "\n",
    "output_notebook()  # For Jupyter inline display\n",
    "\n",
    "# Optional: truncate text if full posts are too long\n",
    "def truncate_text(text, maxlen=500):\n",
    "    return text if len(text) <= maxlen else text[:maxlen] + \"...\"\n",
    "\n",
    "# Truncate if desired\n",
    "original_texts_display = [truncate_text(t) for t in original_texts]\n",
    "generated_texts_display = [truncate_text(t) for t in generated_texts]\n",
    "\n",
    "# Create ColumnDataSources\n",
    "source_original = ColumnDataSource(data=dict(\n",
    "    x=original_pca[:, 0],\n",
    "    y=original_pca[:, 1],\n",
    "    text=original_texts_display,\n",
    "))\n",
    "\n",
    "source_generated = ColumnDataSource(data=dict(\n",
    "    x=generated_pca[:, 0],\n",
    "    y=generated_pca[:, 1],\n",
    "    text=generated_texts_display,\n",
    "))\n",
    "\n",
    "# Create plot\n",
    "p = figure(\n",
    "    title=\"Original vs Generated Embeddings (PCA)\",\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    tools=\"pan,wheel_zoom,reset,save\",\n",
    "    toolbar_location='right'\n",
    ")\n",
    "\n",
    "# Add points\n",
    "p.circle('x', 'y', size=10, source=source_original, color='blue', alpha=0.5, legend_label='Original')\n",
    "p.circle('x', 'y', size=10, source=source_generated, color='red', alpha=0.5, legend_label='Generated')\n",
    "\n",
    "# Add hover tool\n",
    "hover = HoverTool(tooltips=\"\"\"\n",
    "    <div style=\"width:400px; white-space:normal;\">\n",
    "        <strong>Post:</strong><br>@text{safe}\n",
    "    </div>\n",
    "\"\"\")\n",
    "p.add_tools(hover)\n",
    "\n",
    "# Final layout settings\n",
    "p.legend.location = \"top_left\"\n",
    "p.xaxis.axis_label = 'PCA Component 1'\n",
    "p.yaxis.axis_label = 'PCA Component 2'\n",
    "\n",
    "# Show or export\n",
    "output_file(\"outputs_project/original_vs_generated_embeddings_pca.html\")\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295171cf-a0cd-42d1-adf3-e8cdc4930e02",
   "metadata": {},
   "source": [
    "Let's compare original posts to the ones we created with GPT-2. Here we are selecting an index from our new DataFrame and printing the text for both the original and our generated text.\n",
    "\n",
    "**ðŸ”” Question:** What do you make of this specific generated post? Can you tell why the model seems to have created the output that it did?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada03a4f-0005-4dab-94e9-50852c4eac41",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Back to Close Reading\n",
    "\n",
    "Finally, let's read these generated r/amitheasshole posts. Change the `index` number to see different real vs. generated posts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlab",
   "language": "python",
   "name": "dlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
