{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c7cfced-157b-4977-b7d6-bf274f3008cc",
   "metadata": {},
   "source": [
    "# GPT Fintuning - PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24449fb-576c-492f-b085-a9bfae02527b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Package install - restart your kernel after running this!\n",
    "#%pip install torch transformers adjustText gdown plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4e1011-d29c-446a-b042-5cf12d305284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "print(torch.__version__)\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd626f7-0a54-4b62-87cf-da6fc5852e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Set the seed for PyTorch (controls randomness for reproducibility)\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2')\n",
    "\n",
    "# Encode input context to get token IDs\n",
    "input_text = \"PASTE A POST FROM YOUR SUBREDDIT HERE\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# Generate text using the model\n",
    "output = model.generate(input_ids, \n",
    "                        do_sample=True, \n",
    "                        max_length=150, \n",
    "                        repetition_penalty=1.1,\n",
    "                        temperature=.5, \n",
    "                        top_k=30, \n",
    "                        top_p=0.95\n",
    "                        )\n",
    "\n",
    "# Decode the generated IDs to text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faeeeaa1-eee9-4eb3-92c9-3d8555fc34ca",
   "metadata": {},
   "source": [
    "üîî **Question**: Does this output make sense? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a929727-f1d2-43b0-a519-8707f3dc6bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2')\n",
    "embeddings = model.transformer.wte.weight.detach().numpy()\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# CHOOSE WORDS THAT REPRESENT YOUR SUBREDDIT DISCOURSE, add however many you need\n",
    "words = [\"word1\", \"word2\", \"word3\", \"word4\"]\n",
    "word_indices = [tokenizer.encode(word)[0] for word in words]\n",
    "selected_embeddings = embeddings[word_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01f82ef-d101-484f-b064-dc15df4b2205",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ca1f71-d624-42b4-9f7d-3f7940b544e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Using PCA\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings_pca = pca.fit_transform(selected_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829902c2-d49a-4277-b650-f1b9cc59a359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame from the PCA embeddings and labels\n",
    "df_plot = pd.DataFrame({\n",
    "    'x': reduced_embeddings_pca[:, 0],\n",
    "    'y': reduced_embeddings_pca[:, 1],\n",
    "    'label': words\n",
    "})\n",
    "\n",
    "# Create interactive scatter plot with hover labels\n",
    "fig = px.scatter(\n",
    "    df_plot,\n",
    "    x='x',\n",
    "    y='y',\n",
    "    text='label',  # show text labels directly (optional)\n",
    "    title='PCA of GPT-2 Embeddings',\n",
    ")\n",
    "\n",
    "# Make it look nicer\n",
    "fig.update_traces(textposition='top center', marker=dict(size=6, opacity=0.7))\n",
    "fig.update_layout(\n",
    "    xaxis_title='PCA 1',\n",
    "    yaxis_title='PCA 2',\n",
    "    hovermode='closest',\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.write_html(\"outputs_project/gpt2_embeddings_pca.html\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7471de84-6d75-4666-98fd-0b7d4beacedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for comparison\n",
    "reduced_embeddings_pca_original = reduced_embeddings_pca\n",
    "words_original = words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abaa9c5-462f-4d94-b16b-abff0cada126",
   "metadata": {},
   "source": [
    "<a id=\"ft\"></a>\n",
    "\n",
    "# Finetuning GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5327006d-08da-4fb6-92aa-062f556cd89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('../../data/YOUR_DATA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47121d6c-eb73-4ad1-b380-023374dec0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['selftext'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332e7ed4-897f-407d-a751-e9c68b30f953",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ccd160-5cad-4dae-954f-5a1448fb5398",
   "metadata": {},
   "source": [
    "## Commence Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed43f338-c941-4f85-8363-12f19fc2eab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "\n",
    "# Initialize tokenizer with padding token set\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize texts\n",
    "encodings = tokenizer(texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af93c7aa-95ac-4634-a544-8bbb28b798d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        # For language modeling, the labels are the input_ids shifted by one\n",
    "        item[\"labels\"] = item[\"input_ids\"].clone()\n",
    "        return item\n",
    "\n",
    "# Initialize the dataset\n",
    "train_dataset = TextDataset(encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a279ad8f-1df3-4aaf-9d82-8cd44610aba7",
   "metadata": {},
   "source": [
    "## Commence Finetuning\n",
    "\n",
    "‚ö†Ô∏è **Warning:** Even though we are using a small model, the following operation will take *long* on a consumer-grade PC (for reference: around 4 hours on an Apple M2 Pro with 16GB memory). Consider running this on DataHub or Google Colab (check bCourses for a link)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533e8649-bdc5-40c4-bc03-73b95165c023",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2')\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='../../results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    logging_dir='../../logs'\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b59bfcb-2c4c-4be7-81db-af60c0f388c2",
   "metadata": {},
   "source": [
    "If you did run the previous code, make sure to save the model and finetuned tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f25f321-87d9-42f1-8442-4bd0149ffbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = 'ft_model'\n",
    "tokenizer_save_path = 'ft_tokenizer'\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(model_save_path)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(tokenizer_save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f4a9d6-9c4c-4ce2-b3d9-ee16cad56aa4",
   "metadata": {},
   "source": [
    "<a id=\"int\"></a>\n",
    "# Interpreting Model Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b699c0a-f696-4635-80a2-5f00f5630149",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load the tokenizer and model from the Hugging Face Hub\n",
    "ft_tokenizer = GPT2Tokenizer.from_pretrained('ft_tokenizer')\n",
    "ft_model = GPT2LMHeadModel.from_pretrained('ft_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f0b6dd-a986-4937-9412-a1ac794a3abe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Set the seed for PyTorch (controls randomness for reproducibility)\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "def generate_text(model, prompt, do_sample=True, max_length=50, temperature=1, top_k=50, top_p=0.95, repetition_penalty=1.1):\n",
    "    \"\"\"\n",
    "    Generates text based on a given prompt using the specified model.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The fine-tuned model to use for text generation.\n",
    "    - prompt: The initial text to start generating from.\n",
    "    - max_length: Maximum length of the generated text.\n",
    "    - temperature: Sampling temperature for generating text.\n",
    "    - top_k: The number of highest probability vocabulary tokens to keep for top-k filtering.\n",
    "    - top_p: Nucleus sampling's cumulative probability cutoff to keep for top-p filtering.\n",
    "    \n",
    "    Returns:\n",
    "    - generated_text: The generated text as a string.\n",
    "    \"\"\"\n",
    "    # Encode the prompt text to tensor\n",
    "    input_ids = ft_tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    # Generate a sequence of tokens following the prompt\n",
    "    output_ids = ft_model.generate(input_ids, max_length=max_length, \n",
    "                                temperature=temperature, \n",
    "                                do_sample=do_sample, \n",
    "                                top_k=top_k, \n",
    "                                top_p=top_p, \n",
    "                                repetition_penalty=repetition_penalty, \n",
    "                                pad_token_id=ft_tokenizer.eos_token_id)\n",
    "    \n",
    "    # Decode the generated tokens to a string\n",
    "    generated_text = ft_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Prompt to generate text from - play around with this!\n",
    "prompt = \"PASTE A TEXT FROM YOUR SUBREDDIT\"\n",
    "\n",
    "# Generate texts\n",
    "generated_text = generate_text(ft_model, prompt, max_length=150)\n",
    "print(\"Generated text from finetuned model:\", generated_text, '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ce8825-7959-4837-a3dc-3f1793b2124d",
   "metadata": {},
   "source": [
    "## Visualizing the Finetuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b49aa39-11f4-4c9a-9997-c7ee3465a3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = ft_model.transformer.wte.weight.detach().numpy()\n",
    "\n",
    "# CHOOSE THE SAME WORDS THAT REPRESENT YOUR SUBREDDIT DISCOURSE, add however many you need\n",
    "words = [\"word1\", \"word2\", \"word3\", \"word4\"]\n",
    "word_indices = [ft_tokenizer.encode(word)[0] for word in words]\n",
    "selected_embeddings = embeddings[word_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11e798d-2845-4b68-8d19-82919a318cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Using PCA\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings_pca = pca.fit_transform(selected_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa11bf88-fa17-4e8d-b38d-496219089da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create DataFrames for both\n",
    "df_original = pd.DataFrame({\n",
    "    'x': reduced_embeddings_pca_original[:, 0],\n",
    "    'y': reduced_embeddings_pca_original[:, 1],\n",
    "    'label': words_original\n",
    "})\n",
    "\n",
    "df_finetuned = pd.DataFrame({\n",
    "    'x': reduced_embeddings_pca[:, 0],\n",
    "    'y': reduced_embeddings_pca[:, 1],\n",
    "    'label': words\n",
    "})\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Original GPT-2\", \"Finetuned GPT-2\"))\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df_original['x'], y=df_original['y'],\n",
    "        mode='markers+text',\n",
    "        text=df_original['label'],\n",
    "        textposition='top center',\n",
    "        marker=dict(size=6, opacity=0.8),\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df_finetuned['x'], y=df_finetuned['y'],\n",
    "        mode='markers+text',\n",
    "        text=df_finetuned['label'],\n",
    "        textposition='top center',\n",
    "        marker=dict(size=6, opacity=0.8),\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text='PCA of Token Embeddings: Original vs Finetuned GPT-2',\n",
    "    height=500, width=1000\n",
    ")\n",
    "\n",
    "fig.write_html(\"outputs_project/gpt2_comp_embeddings_pca.html\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fff9c0-8b32-403d-92bd-bb74da178dff",
   "metadata": {},
   "source": [
    "# Create Posts Using Finetuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceb8457-2f7c-4f2a-ba08-05325039c768",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, pipeline\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('../../data/YOUR_DATA.csv')\n",
    "\n",
    "# Randomly select 10 entries\n",
    "sample_df = df.sample(n=10, random_state=1).reset_index()  # Keep original index as a column\n",
    "\n",
    "original_texts = [f\"{t}\\n\\n{s}\" for t, s in zip(sample_df['title'], sample_df['selftext'])]\n",
    "titles = sample_df['title'].tolist()\n",
    "\n",
    "# Initialize the generation pipeline\n",
    "generator = pipeline('text-generation', model=ft_model, tokenizer=ft_tokenizer, device=-1)  # CPU usage\n",
    "\n",
    "# Prepare to generate texts\n",
    "generated_texts = []\n",
    "for title, original_text in zip(titles, original_texts):\n",
    "    # Calculate the length of the original post in tokens\n",
    "    target_length = len(ft_tokenizer.encode(original_text))\n",
    "\n",
    "    # Generate a new post of the same length starting from the title\n",
    "    # Ensure to set max_length to the length of the original post\n",
    "    prompt = title\n",
    "    generated = generator(prompt, max_length=target_length, num_return_sequences=1)[0]['generated_text']\n",
    "    generated_texts.append(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4599abc-6e71-4efb-bbec-7244bc9fa63d",
   "metadata": {},
   "source": [
    "Let's save the original and generated posts in a new DataFrame so we can easily compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a819594-d26d-4196-a816-efeb89d5b105",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'original_text': original_texts,\n",
    "    'generated_text': generated_texts,\n",
    "    'title': titles  # Assuming you have a list of titles\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaab800a-3c8a-4ef9-8708-0dfb0d93a39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a37f78e-421f-4d4d-89f7-b99f4a6569fa",
   "metadata": {},
   "source": [
    "## Back to TF-IDF\n",
    "\n",
    "We can use the TF-IDF algorithm to check the similarity between original texts and the ones we generated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7b2974-7925-4c99-aa53-928f26ce4934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Combine original and generated texts into one list for TF-IDF analysis\n",
    "texts = df['original_text'].tolist() + df['generated_text'].tolist()\n",
    "\n",
    "# Initialize a TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the data\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Calculate cosine similarity between original and generated texts\n",
    "# Assuming the first half are originals and the second half are generated\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix[:len(df)], tfidf_matrix[len(df):])\n",
    "\n",
    "# Display similarity results\n",
    "for i, similarity in enumerate(similarity_matrix.diagonal()):\n",
    "    print(f\"Text {i+1} Similarity between original and generated: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5697d176-7f3e-4d1a-87ff-98996717bd91",
   "metadata": {},
   "source": [
    "## Back to Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf5c3d3-d188-44cd-9fa4-251cb1bfb789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load the medium model with word vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample data\n",
    "original_texts = [f\"[Post {i}] {title}\\n\\n{body}\" \n",
    "                  for i, title, body in zip(sample_df['index'], sample_df['title'], sample_df['selftext'])]\n",
    "generated_texts = [f\"[Post {i}] {gen}\" \n",
    "                   for i, gen in zip(sample_df['index'], generated_texts)]\n",
    "\n",
    "# Function to compute average embeddings for a text\n",
    "def get_average_embedding(text, nlp_model):\n",
    "    doc = nlp_model(text)\n",
    "    vectors = [word.vector for word in doc if not word.is_stop and word.has_vector]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros((nlp_model.vocab.vectors_length,))\n",
    "\n",
    "# Compute average embeddings for each set of texts\n",
    "original_embeddings = np.array([get_average_embedding(text, nlp) for text in original_texts])\n",
    "generated_embeddings = np.array([get_average_embedding(text, nlp) for text in generated_texts])\n",
    "\n",
    "# Perform PCA to reduce dimensionality for visualization\n",
    "pca = PCA(n_components=2)\n",
    "original_pca = pca.fit_transform(original_embeddings)\n",
    "generated_pca = pca.transform(generated_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2407f81-cbe1-4c71-a248-3668933ad89b",
   "metadata": {},
   "source": [
    "Plot it with `bokeh`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242cc189-857c-47ef-a68d-5da2e4a21bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "from bokeh.io import output_file\n",
    "import numpy as np\n",
    "\n",
    "output_notebook()  # For Jupyter inline display\n",
    "\n",
    "# Optional: truncate text if full posts are too long\n",
    "def truncate_text(text, maxlen=500):\n",
    "    return text if len(text) <= maxlen else text[:maxlen] + \"...\"\n",
    "\n",
    "# Truncate if desired\n",
    "original_texts_display = [truncate_text(t) for t in original_texts]\n",
    "generated_texts_display = [truncate_text(t) for t in generated_texts]\n",
    "\n",
    "# Create ColumnDataSources\n",
    "source_original = ColumnDataSource(data=dict(\n",
    "    x=original_pca[:, 0],\n",
    "    y=original_pca[:, 1],\n",
    "    text=original_texts_display,\n",
    "))\n",
    "\n",
    "source_generated = ColumnDataSource(data=dict(\n",
    "    x=generated_pca[:, 0],\n",
    "    y=generated_pca[:, 1],\n",
    "    text=generated_texts_display,\n",
    "))\n",
    "\n",
    "# Create plot\n",
    "p = figure(\n",
    "    title=\"Original vs Generated Embeddings (PCA)\",\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    tools=\"pan,wheel_zoom,reset,save\",\n",
    "    toolbar_location='right'\n",
    ")\n",
    "\n",
    "# Add points\n",
    "p.circle('x', 'y', size=10, source=source_original, color='blue', alpha=0.5, legend_label='Original')\n",
    "p.circle('x', 'y', size=10, source=source_generated, color='red', alpha=0.5, legend_label='Generated')\n",
    "\n",
    "# Add hover tool\n",
    "hover = HoverTool(tooltips=\"\"\"\n",
    "    <div style=\"width:400px; white-space:normal;\">\n",
    "        <strong>Post:</strong><br>@text{safe}\n",
    "    </div>\n",
    "\"\"\")\n",
    "p.add_tools(hover)\n",
    "\n",
    "# Final layout settings\n",
    "p.legend.location = \"top_left\"\n",
    "p.xaxis.axis_label = 'PCA Component 1'\n",
    "p.yaxis.axis_label = 'PCA Component 2'\n",
    "\n",
    "# Show or export\n",
    "output_file(\"outputs_project/original_vs_generated_embeddings_pca.html\")\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af316921-bf0a-47b6-a2bc-1c9955c8f4e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Back to Close Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f599a65-0c6f-47b0-8761-d0481d13f51a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = 2\n",
    "print(f\"Title: {df.loc[index]['title']}\")\n",
    "print(\"\\n\")\n",
    "print(f\"Original Text: {df.loc[index, 'original_text']}\")\n",
    "print(\"\\n\")\n",
    "print(f\"Generated Text: {df.loc[index, 'generated_text']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlab",
   "language": "python",
   "name": "dlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
