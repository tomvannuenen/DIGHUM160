{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WClw_VPOmwXd"
   },
   "source": [
    "<img src=\"../../Img/backdrop-wh.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "# Preprocessing\n",
    "* * * \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WClw_VPOmwXd",
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-success\">  \n",
    "    \n",
    "### Learning Objectives \n",
    "    \n",
    "* Understand why textual data needs to be preprocessed.\n",
    "* Engage in common preprocessing tasks, such as lemmatization and phrase modeling.\n",
    "* Distinguish between different Python packages for preprocessing.\n",
    "</div>\n",
    "\n",
    "### Icons Used in This Notebook\n",
    "üîî **Question**: A quick question to help you understand what's going on.<br>\n",
    "üí° **Tip**: How to do something a bit more efficiently or effectively.<br>\n",
    "‚ö†Ô∏è **Warning:** Heads-up about tricky stuff or common mistakes.<br>\n",
    "üí≠ **Reflection**: Reflecting on ethical implications, biases, and social impact in data science.<br>\n",
    "\n",
    "### Sections\n",
    "1. [About This Workshop](#intro)\n",
    "2. [Reading Data with Pandas](#read)\n",
    "3. [Dropping Columns and Missing Values](#drop)\n",
    "4. [Preprocessing Text Data with SpaCy](#clean)\n",
    "5. [Phrase Modeling with Gensim](#gensim)\n",
    "6. [Saving Data](#save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Know the Basics?\n",
    "\n",
    "This notebook expects you to know the basics of Python and the `pandas` package. If you have zero prior exposure to Python, we suggest you take a few days to go over the `Python_Basics.ipynb` notebook in the **Week0** folder first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36-_8ZvxmwXf"
   },
   "source": [
    "<a id='intro'></a>\n",
    "\n",
    "# About This Workshop\n",
    "\n",
    "One of the most common types of data used in machine learning and data science is text data‚Äîranging from social media posts and news articles to customer reviews and interview transcripts. Text is incredibly rich in information, but it doesn‚Äôt come pre-packaged for analysis. Machine learning models can‚Äôt work directly with words‚Äîthey need numbers. That‚Äôs why text preprocessing is a critical first step in any text-based pipeline.\n",
    "\n",
    "In this workshop, you‚Äôll learn how to transform raw text into a structured, numeric format that machine learning algorithms can use. This means building a preprocessing pipeline‚Äîa series of steps that clean, tokenize, and normalize text.\n",
    "\n",
    "Why does this matter? As AI tools become more powerful and accessible, it‚Äôs tempting to rely on them without understanding how they work. But to use AI responsibly‚Äîand interpret its results meaningfully‚Äîyou need to understand the basics. These notebooks builds that conceptual foundation.\n",
    "\n",
    "You‚Äôll practice:\n",
    "1.\tReading, manipulating, and writing .csv files using `pandas`;\n",
    "2.\tPreprocessing text with essential techniques like tokenization, stop-word removal, n-gram extraction, and lemmatization using `spaCy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Don't Be Overwhelmed\n",
    "This module builds on what you learned in Python Basics, especially with pandas. If you‚Äôre newer to Python, some of the code may feel like a lot. That‚Äôs okay. You don‚Äôt need to master every detail right away. Focus on the big picture: how these functions help us turn messy text into analyzable data.\n",
    "\n",
    "**‚ö†Ô∏è Warning**: Some code cells will take time to run. If you see a `*` next to a code cell, it‚Äôs still processing. The good news is that once your pipeline is complete, you can save the output and skip re-running these cells later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NgBDYstgmwX6"
   },
   "source": [
    "<a id='read'></a>\n",
    "\n",
    "# Recap: Our Data\n",
    "\n",
    "The first step is to read the data we are going to work with into Python so that we can work with it.\n",
    "\n",
    "We're using the dataset taken from the subreddit [r/amitheasshole](http://www.reddit.com/r/AmItheAsshole). \n",
    "\n",
    "The subreddit describes itself as follows:\n",
    "\n",
    "<img src=\"../../img/aita_desc.png\" alt=\"Am I The Asshole - description\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subreddit has structures in place that the community follow to come to a decision about the situation. First, OP (original poster) writes up the situation, asking AITA (Am I The Asshole). In response, for eighteen hours, the community of the subreddit will respond to the post with one of five judgments: YTA (You‚Äôre The Asshole), NTA (Not The Asshole), ESH (Everyone Sucks Here), NAH (No Assholes Here), or INFO (Not Enough Info).\n",
    "\n",
    "üí° **Tip**: For more info on the subreddit, see [here](https://www.inverse.com/culture/am-i-the-asshole/amp). \n",
    "\n",
    "First, we have to read the data. We'll use a subset of the full dataset consisting of the top most popular posts, the assumption being that this will yield the most interesting results (`aita_sub_top_sm.csv`). We use `pd.read_csv()` to import the .csv file as a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure data files are downloaded\n",
    "!git lfs pull --include=\"data/*.csv\"\n",
    "print(\"‚úÖ Data ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2525,
     "status": "ok",
     "timestamp": 1647459522220,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": 300
    },
    "id": "bIB1SsShP9iL",
    "outputId": "ab2ec235-c74b-4f81-f7dc-d41825605164"
   },
   "outputs": [],
   "source": [
    "# Import the pandas package\n",
    "import pandas as pd \n",
    "\n",
    "# Read the csv file\n",
    "df = pd.read_csv('../../data/aita_top_submissions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idint</th>\n",
       "      <th>idstr</th>\n",
       "      <th>created</th>\n",
       "      <th>self</th>\n",
       "      <th>nsfw</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>selftext</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>distinguish</th>\n",
       "      <th>textlen</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>flair_text</th>\n",
       "      <th>flair_css_class</th>\n",
       "      <th>augmented_at</th>\n",
       "      <th>augmented_count</th>\n",
       "      <th>pp_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>427576402</td>\n",
       "      <td>t3_72kg2a</td>\n",
       "      <td>1506433689</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Ritsku</td>\n",
       "      <td>AITA for breaking up with my girlfriend becaus...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My girlfriend recently went to the beach with ...</td>\n",
       "      <td>679.0</td>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4917.0</td>\n",
       "      <td>434.0</td>\n",
       "      <td>no a--holes here</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>girlfriend recently went beach friends tiny bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>551887974</td>\n",
       "      <td>t3_94kvhi</td>\n",
       "      <td>1533404095</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>hhhhhhffff678</td>\n",
       "      <td>AITA for banning smoking in my house and telli...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My parents smoke like chimneys. I used to as w...</td>\n",
       "      <td>832.0</td>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2076.0</td>\n",
       "      <td>357.0</td>\n",
       "      <td>asshole</td>\n",
       "      <td>ass</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>parents smoke like chimneys quit wife got youn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>552654542</td>\n",
       "      <td>t3_951az2</td>\n",
       "      <td>1533562299</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>creepatthepool</td>\n",
       "      <td>AITA? Creep wears skimpy bathing suit to pool</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hi guys. Throwaway for obv reasons.\\n\\nI'm a f...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1741.0</td>\n",
       "      <td>335.0</td>\n",
       "      <td>Shitpost</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hi guys throwaway obv reasons i'm female child...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       idint      idstr     created  self  nsfw          author  \\\n",
       "0  427576402  t3_72kg2a  1506433689   1.0   0.0          Ritsku   \n",
       "1  551887974  t3_94kvhi  1533404095   1.0   0.0   hhhhhhffff678   \n",
       "2  552654542  t3_951az2  1533562299   1.0   0.0  creepatthepool   \n",
       "\n",
       "                                               title  url  \\\n",
       "0  AITA for breaking up with my girlfriend becaus...  NaN   \n",
       "1  AITA for banning smoking in my house and telli...  NaN   \n",
       "2      AITA? Creep wears skimpy bathing suit to pool  NaN   \n",
       "\n",
       "                                            selftext  score      subreddit  \\\n",
       "0  My girlfriend recently went to the beach with ...  679.0  AmItheAsshole   \n",
       "1  My parents smoke like chimneys. I used to as w...  832.0  AmItheAsshole   \n",
       "2  Hi guys. Throwaway for obv reasons.\\n\\nI'm a f...   23.0  AmItheAsshole   \n",
       "\n",
       "  distinguish  textlen  num_comments        flair_text flair_css_class  \\\n",
       "0         NaN   4917.0         434.0  no a--holes here             NaN   \n",
       "1         NaN   2076.0         357.0           asshole             ass   \n",
       "2         NaN   1741.0         335.0          Shitpost             NaN   \n",
       "\n",
       "   augmented_at  augmented_count  \\\n",
       "0           NaN              NaN   \n",
       "1           NaN              NaN   \n",
       "2           NaN              NaN   \n",
       "\n",
       "                                             pp_text  \n",
       "0  girlfriend recently went beach friends tiny bi...  \n",
       "1  parents smoke like chimneys quit wife got youn...  \n",
       "2  hi guys throwaway obv reasons i'm female child...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at look and the shape, top rows, and columns of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 18)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 773
    },
    "executionInfo": {
     "elapsed": 313,
     "status": "ok",
     "timestamp": 1647459542614,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": 300
    },
    "id": "TkI-FuY2mwX6",
    "outputId": "87472d59-a42b-4a76-ff02-3aaa0f474e78",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idint</th>\n",
       "      <th>idstr</th>\n",
       "      <th>created</th>\n",
       "      <th>self</th>\n",
       "      <th>nsfw</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>selftext</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>distinguish</th>\n",
       "      <th>textlen</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>flair_text</th>\n",
       "      <th>flair_css_class</th>\n",
       "      <th>augmented_at</th>\n",
       "      <th>augmented_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>427576402</td>\n",
       "      <td>t3_72kg2a</td>\n",
       "      <td>1506433689</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Ritsku</td>\n",
       "      <td>AITA for breaking up with my girlfriend becaus...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My girlfriend recently went to the beach with ...</td>\n",
       "      <td>679.0</td>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4917.0</td>\n",
       "      <td>434.0</td>\n",
       "      <td>no a--holes here</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>551887974</td>\n",
       "      <td>t3_94kvhi</td>\n",
       "      <td>1533404095</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>hhhhhhffff678</td>\n",
       "      <td>AITA for banning smoking in my house and telli...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My parents smoke like chimneys. I used to as w...</td>\n",
       "      <td>832.0</td>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2076.0</td>\n",
       "      <td>357.0</td>\n",
       "      <td>asshole</td>\n",
       "      <td>ass</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>552654542</td>\n",
       "      <td>t3_951az2</td>\n",
       "      <td>1533562299</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>creepatthepool</td>\n",
       "      <td>AITA? Creep wears skimpy bathing suit to pool</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hi guys. Throwaway for obv reasons.\\n\\nI'm a f...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1741.0</td>\n",
       "      <td>335.0</td>\n",
       "      <td>Shitpost</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>556350346</td>\n",
       "      <td>t3_978ioa</td>\n",
       "      <td>1534254641</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Pauly104</td>\n",
       "      <td>AITA for eating steak in front of my vegan GF?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yesterday night, me and my GF decided to go ou...</td>\n",
       "      <td>1011.0</td>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>NaN</td>\n",
       "      <td>416.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>not the a-hole</td>\n",
       "      <td>not</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>560929656</td>\n",
       "      <td>t3_99yo3c</td>\n",
       "      <td>1535126620</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ThatSpencerGuy</td>\n",
       "      <td>AITA for not wanting to cook my mother-in-law ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My wife and I are vegetarians, much to my in-l...</td>\n",
       "      <td>349.0</td>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1158.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>not the a-hole</td>\n",
       "      <td>not</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       idint      idstr     created  self  nsfw          author  \\\n",
       "0  427576402  t3_72kg2a  1506433689   1.0   0.0          Ritsku   \n",
       "1  551887974  t3_94kvhi  1533404095   1.0   0.0   hhhhhhffff678   \n",
       "2  552654542  t3_951az2  1533562299   1.0   0.0  creepatthepool   \n",
       "3  556350346  t3_978ioa  1534254641   1.0   0.0        Pauly104   \n",
       "4  560929656  t3_99yo3c  1535126620   1.0   0.0  ThatSpencerGuy   \n",
       "\n",
       "                                               title  url  \\\n",
       "0  AITA for breaking up with my girlfriend becaus...  NaN   \n",
       "1  AITA for banning smoking in my house and telli...  NaN   \n",
       "2      AITA? Creep wears skimpy bathing suit to pool  NaN   \n",
       "3     AITA for eating steak in front of my vegan GF?  NaN   \n",
       "4  AITA for not wanting to cook my mother-in-law ...  NaN   \n",
       "\n",
       "                                            selftext   score      subreddit  \\\n",
       "0  My girlfriend recently went to the beach with ...   679.0  AmItheAsshole   \n",
       "1  My parents smoke like chimneys. I used to as w...   832.0  AmItheAsshole   \n",
       "2  Hi guys. Throwaway for obv reasons.\\n\\nI'm a f...    23.0  AmItheAsshole   \n",
       "3  Yesterday night, me and my GF decided to go ou...  1011.0  AmItheAsshole   \n",
       "4  My wife and I are vegetarians, much to my in-l...   349.0  AmItheAsshole   \n",
       "\n",
       "  distinguish  textlen  num_comments        flair_text flair_css_class  \\\n",
       "0         NaN   4917.0         434.0  no a--holes here             NaN   \n",
       "1         NaN   2076.0         357.0           asshole             ass   \n",
       "2         NaN   1741.0         335.0          Shitpost             NaN   \n",
       "3         NaN    416.0         380.0    not the a-hole             not   \n",
       "4         NaN   1158.0         360.0    not the a-hole             not   \n",
       "\n",
       "   augmented_at  augmented_count  \n",
       "0           NaN              NaN  \n",
       "1           NaN              NaN  \n",
       "2           NaN              NaN  \n",
       "3           NaN              NaN  \n",
       "4           NaN              NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1639763489064,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "7oZfTJi2rA6Z",
    "outputId": "88d7211b-3e6e-447c-87da-fcffadc71a58"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['idint',\n",
       " 'idstr',\n",
       " 'created',\n",
       " 'self',\n",
       " 'nsfw',\n",
       " 'author',\n",
       " 'title',\n",
       " 'url',\n",
       " 'selftext',\n",
       " 'score',\n",
       " 'subreddit',\n",
       " 'distinguish',\n",
       " 'textlen',\n",
       " 'num_comments',\n",
       " 'flair_text',\n",
       " 'flair_css_class',\n",
       " 'augmented_at',\n",
       " 'augmented_count']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This allows you to quickly see which columns you have\n",
    "list(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzrQAWgVqgwD"
   },
   "source": [
    "This particular dataset only includes the original posts in the subreddit (so not the comments on the posts). \n",
    "\n",
    "There is one row per post in the dataset. The columns are as follows:\n",
    "\n",
    "-  `idstr`: ID of the post.\n",
    "- `created`: the time of the post's creation.\n",
    "- `author`: Reddit author of the post.\n",
    "- `title`: Title of the post.\n",
    "- `selftext`: Text of the post.\n",
    "- `score`: Amount of upvotes minus downvotes.\n",
    "- `textlen`: Amount of words.\n",
    "- `num_comments`: Amount of comments.\n",
    "- `nsfw`: Flag for NSFW content.\n",
    "- `flair_text`: A 'tag' that users within a subreddit can add.\n",
    "\n",
    "üí≠ **Reflection**: Which of these columns could contain interesting data for your research purposes? How could you make use of things like the number of comments, NSFW flags, or even text length, in relation to a discourse community?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaH8ZNOv50it"
   },
   "source": [
    "## Removing Missing Values\n",
    "\n",
    "Data collected from the internet typically has missing values and other inconsistencies. It's typically called \"dirty data\", and it needs to be cleaned up first.\n",
    "\n",
    "First, we want to select the rows of deleted posts. On Reddit, removed posts get flagged as \"[removed]\" or \"[deleted]\". We should remove these posts, since they'll lack any text. \n",
    "\n",
    "We can do this using the [`isin()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isin.html) method. We run it on the `selftext` column and include a list of phrases that indicate a removed post ‚Äì that is, \"[removed]\" and \"[deleted]\".\n",
    "\n",
    "The following line of code will select all lines that do not have 'removed' or 'deleted' in the post's text. The `~` means 'not'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16309, 18)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select all rows that don't have '[removed]' or '[deleted]'\n",
    "df = df.loc[~df['selftext'].isin(['[removed]', '[deleted]' ]),:]\n",
    "\n",
    "# Select all rows that have >3 characters in selftext\n",
    "df = df.loc[df['selftext'].str.len() > 3]\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîî **Question**: How many posts are left in the dataset? How many did we lose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Null Values\n",
    "\n",
    "Next, we need to drop **null values**. These are values that are totally missing. In this case, the web scraper may have been unable to extract text for the post. They are replaced with the value **NaN**, which stands for a null value in `pandas`. We need to deal with null values in any column that we plan to use for analysis, which in this case is the `selftext` column. We can use [`dropna()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html?highlight=dropna#pandas.DataFrame.dropna) to remove the rows with null values in the target column. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the `dropna()` method on the dataframe. We use the argument `subset`, which you set to `'selftext'`). Check the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html) if you want a reminder of how this works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['selftext'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîî **Question**: How many posts are left in the dataset? How many did we lose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16309, 18)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-Vy6u-1ZWy9"
   },
   "source": [
    "<a id='preprocess'></a>\n",
    "\n",
    "# Preprocessing Text Data with SpaCy\n",
    "\n",
    "Text data collected in the real world is always going to be variable, which poses a challenge for analysis. But by reducing some of this variation, we can help improve our results. For example, if we are counting instances of the word `\"weather\"` in text, we might want the strings `\"weather\"`, `\"weather.\"`, and `\"Weather\"` to all be counted as instances of the same word. However, in raw text form, these would be treated as separate strings. By performing text cleaning, we can standardize these cases and make our data easier to analyze. Some common preprocessing steps are:\n",
    "\n",
    "- Removing punctuation\n",
    "- Removing URLs\n",
    "- Removing stopwords (non-content words like \"a\", \"the\", \"is\", etc.)\n",
    "- Lowercasing\n",
    "- Tokenization (e.g., splitting a sentence into distinct \"chunks\" or \"tokens\")\n",
    "- Lemmatization, or changing words to 'dictionary form' (e.g., runs, running, ran -> run)\n",
    "\n",
    "Fortunately, we don't need to code every one of these steps. Instead, we will use a package called [spaCy](https://spacy.io/) to do these things. If the text you'd like to process is general-purpose English language text (i.e., not domain-specific, like medical literature), `spaCy` is ready to use out-of-the-box. \n",
    "\n",
    "However, we need to do one more installation: the underlying language \"pipeline\" that spaCy uses. This pipeline contains all the rules and code spaCy uses to perform text preprocessing. Using this pipeline, spaCy will extract information about **attributes** of our text: the tokens, lemmas, stopwords, and so on.\n",
    "\n",
    "The most common pipeline to use is the [`en_core_web_sm`](https://spacy.io/models/en/#en_core_web_sm) pipeline, which is the English (\"en\") model, which contains many \"core\" functions, is developed partially with \"web\" data, and is the small version (\"sm\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy and en_core_web_sm are already installed.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    print(\"Spacy and en_core_web_sm are already installed.\")\n",
    "except:\n",
    "    # Install spacy if it's not installed\n",
    "    %pip install -U pydantic spacy\n",
    "    \n",
    "    # Install en_core_web_sm if it's not installed\n",
    "    import spacy\n",
    "    from spacy.cli.download import download\n",
    "\n",
    "    download('en_core_web_sm')\n",
    "    nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this `nlp` object--holding a spaCy pipeline--to parse a Reddit post in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 1405,
     "status": "ok",
     "timestamp": 1647462821529,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": 300
    },
    "id": "XYssvJqaKnua",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÄúAre we the assholes‚Äù really because my husband made the decision with me.\n",
      "\n",
      "My father passed away a few months ago and left a pretty sizeable estate behind. The majority went to my sister and me, with an equal amount of money to each grandchild (or so I thought.) \n",
      "\n",
      "My kids are grown so I never really thought to reach out about the money because the executor was handling it. My daughter never mentioned it, but she‚Äôs always been frugal so I figured she didn‚Äôt want to discuss it and had some plan for it.\n",
      "\n",
      "Recently we had a family meal and my son brought up the inheritance, and my daughter revealed that she had received nothing and didn‚Äôt know that any of her cousins or her brother had. It is very clear to us that she was cut from the will because my father always disapproved of my son in law.\n",
      "\n",
      "My husband and I talked it over and decided to give a portion of our inheritance to our daughter and her husband to match the amount given to the other grandkids. We are still working with a lawyer to figure out the best way to give her the money so the transfer hasn‚Äôt actually happened yet, but we will definitely be doing this. Our son is very upset because he feels that this is a gift coming from us, and it is unfair that we are not also giving him money. He feels that since my father chose not to give money to my daughter, that‚Äôs the end of it and that any money we give our daughter is now an unfair gift. Are we the assholes for doing this and not giving our son anything?\n"
     ]
    }
   ],
   "source": [
    "# Parse a reddit post in the dataset\n",
    "parsed_post = nlp(df['selftext'][1000])\n",
    "print(parsed_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n94uTq2ZK3X-"
   },
   "source": [
    "The base text looks the same, but we can take a closer look at `parsed_post` to see what happened under the hood. A lot of the information SpaCy has gathered operates on each **token** in the text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SpaCy Attributes\n",
    "\n",
    "When we ran spaCy on our post, it automatically tokenized all our data. If we loop over our `parsed_post`, we will loop over all of these tokens. We can now look at some **attributes** that SpaCy has extracted for each token. \n",
    "\n",
    "We will save these attributes in a list of dictionary items, then put that list into a DataFrame like we did in Python Fundamentals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>dep</th>\n",
       "      <th>shape</th>\n",
       "      <th>is_alpha</th>\n",
       "      <th>is_stop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>‚Äú</td>\n",
       "      <td>\"</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>``</td>\n",
       "      <td>punct</td>\n",
       "      <td>‚Äú</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Are</td>\n",
       "      <td>be</td>\n",
       "      <td>AUX</td>\n",
       "      <td>VBP</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>Xxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>we</td>\n",
       "      <td>we</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>xx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "      <td>det</td>\n",
       "      <td>xxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>assholes</td>\n",
       "      <td>asshole</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NNS</td>\n",
       "      <td>attr</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>giving</td>\n",
       "      <td>give</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBG</td>\n",
       "      <td>conj</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>our</td>\n",
       "      <td>our</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>poss</td>\n",
       "      <td>xxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>son</td>\n",
       "      <td>son</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>dative</td>\n",
       "      <td>xxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>anything</td>\n",
       "      <td>anything</td>\n",
       "      <td>PRON</td>\n",
       "      <td>NN</td>\n",
       "      <td>dobj</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>.</td>\n",
       "      <td>punct</td>\n",
       "      <td>?</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>319 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         text     lemma    pos   tag     dep shape  is_alpha  is_stop\n",
       "0           ‚Äú         \"  PUNCT    ``   punct     ‚Äú     False    False\n",
       "1         Are        be    AUX   VBP    ROOT   Xxx      True     True\n",
       "2          we        we   PRON   PRP   nsubj    xx      True     True\n",
       "3         the       the    DET    DT     det   xxx      True     True\n",
       "4    assholes   asshole   NOUN   NNS    attr  xxxx      True    False\n",
       "..        ...       ...    ...   ...     ...   ...       ...      ...\n",
       "314    giving      give   VERB   VBG    conj  xxxx      True    False\n",
       "315       our       our   PRON  PRP$    poss   xxx      True     True\n",
       "316       son       son   NOUN    NN  dative   xxx      True    False\n",
       "317  anything  anything   PRON    NN    dobj  xxxx      True     True\n",
       "318         ?         ?  PUNCT     .   punct     ?     False    False\n",
       "\n",
       "[319 rows x 8 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an empty list to store token attributes\n",
    "token_data = []\n",
    "\n",
    "# Iterate over tokens and extract attributes\n",
    "for token in parsed_post:\n",
    "    token_data.append({\n",
    "        \"text\": token.text,\n",
    "        \"lemma\": token.lemma_,\n",
    "        \"pos\": token.pos_,\n",
    "        \"tag\": token.tag_,\n",
    "        \"dep\": token.dep_,\n",
    "        \"shape\": token.shape_,\n",
    "        \"is_alpha\": token.is_alpha,\n",
    "        \"is_stop\": token.is_stop,\n",
    "    })\n",
    "\n",
    "# Create a pandas DataFrame from the token data\n",
    "token_df = pd.DataFrame(token_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "token_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynLOmKULRtwz"
   },
   "source": [
    "As you can see, `spaCy` does a *lot* of work. Let's have a look at the [documentation](https://spacy.io/api/attributes) to see which attributes we are looking at here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OA0wy-ttNE7u"
   },
   "source": [
    "## Preprocessing all data\n",
    "Now we can scale up this process to our whole dataset. Up until this point, we have run the preprocessing pipeline on a single post, but we want to automate our code to process all posts at once. \n",
    "\n",
    "To do this, Let's define a helper function that we'll use for preprocessing. The `process_text` function will use `spaCy` to:\n",
    "\n",
    "- Iterate over the dataframe.\n",
    "- Segment the threads into individual sentences.\n",
    "- Remove punctuation, unneccessary tokens such as stopwords, and excess whitespace.\n",
    "- Lemmatize the text.\n",
    "\n",
    "The function will automate the preprocessing for the posts in this dataset. Don't worry too much about deciphering each line of code. The main goal of this function is to do a lot of the preprocessing for you so that you can use the text in analysis going forward. \n",
    "\n",
    "**You can find this function in the `1_Preprocessing_Project.ipynb` notebook to use on your own data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    \"\"\"Function to process a single text string.\"\"\"\n",
    "\n",
    "    text = text.replace('\\n', '')\n",
    "    parsed = nlp(text, disable=[\"tok2vec\", \"ner\"])\n",
    "\n",
    "    # Gather lowercased, lemmatized tokens that are not punctuation, space, or digit\n",
    "    tokens = [\n",
    "        token.lemma_.lower() if token.lemma_ != '-PRON-'\n",
    "        else token.lower_ \n",
    "        for token in parsed \n",
    "        if not (token.is_punct or token.is_space or token.is_digit)\n",
    "    ]\n",
    "\n",
    "    # Remove specific lemmatizations, and words that are not nouns or adjectives\n",
    "    tokens = [\n",
    "        lemma\n",
    "        for lemma in tokens\n",
    "        if not lemma in [\"'s\",  \"‚Äôs\", \"‚Äô\"]\n",
    "    ]\n",
    "\n",
    "    # Remove stop words\n",
    "    tokens = [\n",
    "        token \n",
    "        for token in tokens \n",
    "        if token not in spacy.lang.en.stop_words.STOP_WORDS\n",
    "    ]\n",
    "\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also create the `preprocess` function, which will call the above helper function and `apply()` it over our dataframe.\n",
    "\n",
    "Note that we are setting a parameter `text_col` which refers to the name of the text column we are expecting. We do this so it's easy to run `preprocess` when the text column has another name. This is the case when you're working with .CSV files with comments--the text column will not be called `selftext` but `body`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def preprocess(df, text_col='selftext'):\n",
    "    \"\"\"Preprocessing function to apply to a dataframe.\"\"\"\n",
    "    # Enable tqdm for pandas\n",
    "    tqdm.pandas(desc=\"Processing text\")\n",
    "    \n",
    "    df['pp_text'] = df[text_col].progress_apply(process_text)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run `preprocess()` over our dataframe. **This will take a few minutes to run.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing text: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16309/16309 [08:49<00:00, 30.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# This may take a while\n",
    "df = preprocess(df, text_col='selftext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idint</th>\n",
       "      <th>idstr</th>\n",
       "      <th>created</th>\n",
       "      <th>self</th>\n",
       "      <th>nsfw</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>selftext</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>distinguish</th>\n",
       "      <th>textlen</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>flair_text</th>\n",
       "      <th>flair_css_class</th>\n",
       "      <th>augmented_at</th>\n",
       "      <th>augmented_count</th>\n",
       "      <th>pp_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>427576402</td>\n",
       "      <td>t3_72kg2a</td>\n",
       "      <td>1506433689</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Ritsku</td>\n",
       "      <td>AITA for breaking up with my girlfriend becaus...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My girlfriend recently went to the beach with ...</td>\n",
       "      <td>679.0</td>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4917.0</td>\n",
       "      <td>434.0</td>\n",
       "      <td>no a--holes here</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>girlfriend recently went beach friends tiny bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>551887974</td>\n",
       "      <td>t3_94kvhi</td>\n",
       "      <td>1533404095</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>hhhhhhffff678</td>\n",
       "      <td>AITA for banning smoking in my house and telli...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My parents smoke like chimneys. I used to as w...</td>\n",
       "      <td>832.0</td>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2076.0</td>\n",
       "      <td>357.0</td>\n",
       "      <td>asshole</td>\n",
       "      <td>ass</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>parents smoke like chimneys quit wife got youn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>552654542</td>\n",
       "      <td>t3_951az2</td>\n",
       "      <td>1533562299</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>creepatthepool</td>\n",
       "      <td>AITA? Creep wears skimpy bathing suit to pool</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hi guys. Throwaway for obv reasons.\\n\\nI'm a f...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1741.0</td>\n",
       "      <td>335.0</td>\n",
       "      <td>Shitpost</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hi guys throwaway obv reasons i'm female child...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>556350346</td>\n",
       "      <td>t3_978ioa</td>\n",
       "      <td>1534254641</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Pauly104</td>\n",
       "      <td>AITA for eating steak in front of my vegan GF?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yesterday night, me and my GF decided to go ou...</td>\n",
       "      <td>1011.0</td>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>NaN</td>\n",
       "      <td>416.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>not the a-hole</td>\n",
       "      <td>not</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yesterday night gf decided eat vegan day going...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>560929656</td>\n",
       "      <td>t3_99yo3c</td>\n",
       "      <td>1535126620</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ThatSpencerGuy</td>\n",
       "      <td>AITA for not wanting to cook my mother-in-law ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My wife and I are vegetarians, much to my in-l...</td>\n",
       "      <td>349.0</td>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1158.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>not the a-hole</td>\n",
       "      <td>not</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wife vegetarians laws vocal annoyance year vis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       idint      idstr     created  self  nsfw          author  \\\n",
       "0  427576402  t3_72kg2a  1506433689   1.0   0.0          Ritsku   \n",
       "1  551887974  t3_94kvhi  1533404095   1.0   0.0   hhhhhhffff678   \n",
       "2  552654542  t3_951az2  1533562299   1.0   0.0  creepatthepool   \n",
       "3  556350346  t3_978ioa  1534254641   1.0   0.0        Pauly104   \n",
       "4  560929656  t3_99yo3c  1535126620   1.0   0.0  ThatSpencerGuy   \n",
       "\n",
       "                                               title  url  \\\n",
       "0  AITA for breaking up with my girlfriend becaus...  NaN   \n",
       "1  AITA for banning smoking in my house and telli...  NaN   \n",
       "2      AITA? Creep wears skimpy bathing suit to pool  NaN   \n",
       "3     AITA for eating steak in front of my vegan GF?  NaN   \n",
       "4  AITA for not wanting to cook my mother-in-law ...  NaN   \n",
       "\n",
       "                                            selftext   score      subreddit  \\\n",
       "0  My girlfriend recently went to the beach with ...   679.0  AmItheAsshole   \n",
       "1  My parents smoke like chimneys. I used to as w...   832.0  AmItheAsshole   \n",
       "2  Hi guys. Throwaway for obv reasons.\\n\\nI'm a f...    23.0  AmItheAsshole   \n",
       "3  Yesterday night, me and my GF decided to go ou...  1011.0  AmItheAsshole   \n",
       "4  My wife and I are vegetarians, much to my in-l...   349.0  AmItheAsshole   \n",
       "\n",
       "  distinguish  textlen  num_comments        flair_text flair_css_class  \\\n",
       "0         NaN   4917.0         434.0  no a--holes here             NaN   \n",
       "1         NaN   2076.0         357.0           asshole             ass   \n",
       "2         NaN   1741.0         335.0          Shitpost             NaN   \n",
       "3         NaN    416.0         380.0    not the a-hole             not   \n",
       "4         NaN   1158.0         360.0    not the a-hole             not   \n",
       "\n",
       "   augmented_at  augmented_count  \\\n",
       "0           NaN              NaN   \n",
       "1           NaN              NaN   \n",
       "2           NaN              NaN   \n",
       "3           NaN              NaN   \n",
       "4           NaN              NaN   \n",
       "\n",
       "                                             pp_text  \n",
       "0  girlfriend recently went beach friends tiny bi...  \n",
       "1  parents smoke like chimneys quit wife got youn...  \n",
       "2  hi guys throwaway obv reasons i'm female child...  \n",
       "3  yesterday night gf decided eat vegan day going...  \n",
       "4  wife vegetarians laws vocal annoyance year vis...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí≠ Reflection\n",
    "\n",
    "In one of the lines of our `preprocess` function, we are getting rid of stopwords. These are the most common words in any language (like articles, prepositions, pronouns, conjunctions), such as \"the\", \"a\", \"an\", and \"so\".\n",
    "\n",
    "Stopwords are considered \"low-level information\": the removal of these words does not show any negative consequences on the model we train for our task. It also reduces the dataset size and thus reduces the training time due to the fewer number of tokens involved in the training.\n",
    "\n",
    "Think about what **ethical implications** there could be for removing stopwords. What kind of information are we losing here? What would be a situation in which you would want to keep stopwords?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNQTjb50E1Fm"
   },
   "source": [
    "<a id='gensim'></a>\n",
    "\n",
    "# Phrase Modeling with Gensim\n",
    "\n",
    "Many kinds of NLP methods work better when using **N-grams**. An n-gram treats small groups of words as tokens rather than single words. This allows words that frequently appearing together to be concatenated (e.g. \"new york\" means something different and more specific than \"new\" and \"york\" separately). We most commonly use **bigrams** (2-word phrases) and **trigrams** (3-word phrases).\n",
    "\n",
    "**Phrase modeling** is an approach to learning combinations of tokens that together represent meaningful multi-word concepts. So rather than treating every pair of words as a n-gram, we look for pairs that occur together frequently and identify those as n-grams. This constrains the token space by limiting the number of multi-word tokens, but requires information about what words co-occur together frequently, which is where **phrase models** come in. We can develop phrase models by looping over the the words in our lemmatized dataset and looking for words that co-occur (i.e., appear one after another) together much more frequently than you would expect them to by random chance. \n",
    "\n",
    "`gensim` is a popular natural language processing package. We will use it in later lessons for topic modeling and word embeddings. It also contains a [`Phrases`](https://radimrehurek.com/gensim/models/phrases.html) model that implements phrase modeling for identifying bigrams, trigrams, quadgrams, etc. `Phrases` detects phrases based on collocation counts. It builds a model of input text that you then can use on other data.\n",
    "\n",
    "`gensim` detects a bigram if a scoring function for two words exceeds a threshold. The two important arguments to `Phrases` are `min_count` and `threshold`. The higher the values of these parameters, the harder it is for words to be combined to bigrams. We can change the value of these parameters to fine-tune our model. Try changing `min count` and `threshold` below. How does that change the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 215,
     "status": "ok",
     "timestamp": 1639840527645,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "B_VI6UYEBirn",
    "outputId": "d38f9936-85b7-4d98-a847-f4da9a438ecf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['new_york', 'is', 'great'],\n",
       " ['new_york', 'is', 'in', 'the', 'united', 'states'],\n",
       " ['i', 'love', 'to', 'stay', 'in', 'new_york'],\n",
       " ['people', 'visit', 'the', 'united', 'states']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "# \"Documents\"\n",
    "docs = ['new york is great',\n",
    "        'new york is in the united states',\n",
    "        'i love to stay in new york',\n",
    "        'people visit the united states']\n",
    "# Rudimentary tokenization\n",
    "tokens = [doc.split(\" \") for doc in docs]\n",
    "# Create bigrams\n",
    "bigram = Phrases(tokens, min_count=2, threshold=3, delimiter='_')\n",
    "# Freeze bigrams and apply to data\n",
    "bigram_phraser = Phraser(bigram)\n",
    "[bigram_phraser[token] for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a bigram and trigram model for our data. Starting from the preprocessed lemmas from the `preprocess()` function above, we can use the `gensim` models to identify bigrams and trigrams in the dataset. Again, the `min_count` and `threshold` arguments can be modified to change the output of the model. \n",
    "\n",
    "In the code below we make a bigram model using the gensim `Phrases` object, and then build a trigram model on top of that bigram model. Finally, we use the lemmas from the preprocessed text to make a trigram model of the data. We are processing the whole dataset in this cell, so it may take a little while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 63667,
     "status": "ok",
     "timestamp": 1639846696683,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "MxeZ7mc9DeMj",
    "outputId": "a478fee9-f5b1-4562-e3a6-860e244e5719"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16309/16309 [00:00<00:00, 124406.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training bigram model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training trigram model...\n",
      "Forming trigrams...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating trigrams: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16309/16309 [00:02<00:00, 7394.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create bigram and trigram models\n",
    "print(\"Tokenizing documents...\")\n",
    "tokens = [doc.split(\" \") for doc in tqdm(df['pp_text'], desc=\"Tokenizing\")]\n",
    "\n",
    "print(\"Training bigram model...\")\n",
    "bigram = Phrases(tokens, min_count=10, threshold=100)\n",
    "\n",
    "print(\"Training trigram model...\")\n",
    "trigram = Phrases(bigram[tokens], min_count=10, threshold=50)  \n",
    "\n",
    "bigram_phraser = Phraser(bigram)\n",
    "trigram_phraser = Phraser(trigram)\n",
    "\n",
    "# Form trigrams with progress\n",
    "print(\"Forming trigrams...\")\n",
    "df['pp_text'] = [' '.join(trigram_phraser[bigram_phraser[doc]]) \n",
    "                 for doc in tqdm(tokens, desc=\"Creating trigrams\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idint</th>\n",
       "      <th>idstr</th>\n",
       "      <th>created</th>\n",
       "      <th>self</th>\n",
       "      <th>nsfw</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>selftext</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>distinguish</th>\n",
       "      <th>textlen</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>flair_text</th>\n",
       "      <th>flair_css_class</th>\n",
       "      <th>augmented_at</th>\n",
       "      <th>augmented_count</th>\n",
       "      <th>pp_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>427576402</td>\n",
       "      <td>t3_72kg2a</td>\n",
       "      <td>1506433689</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Ritsku</td>\n",
       "      <td>AITA for breaking up with my girlfriend becaus...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My girlfriend recently went to the beach with ...</td>\n",
       "      <td>679.0</td>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4917.0</td>\n",
       "      <td>434.0</td>\n",
       "      <td>no a--holes here</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>girlfriend recently went beach friends tiny bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>551887974</td>\n",
       "      <td>t3_94kvhi</td>\n",
       "      <td>1533404095</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>hhhhhhffff678</td>\n",
       "      <td>AITA for banning smoking in my house and telli...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My parents smoke like chimneys. I used to as w...</td>\n",
       "      <td>832.0</td>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2076.0</td>\n",
       "      <td>357.0</td>\n",
       "      <td>asshole</td>\n",
       "      <td>ass</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>parents smoke like chimneys quit wife got youn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>552654542</td>\n",
       "      <td>t3_951az2</td>\n",
       "      <td>1533562299</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>creepatthepool</td>\n",
       "      <td>AITA? Creep wears skimpy bathing suit to pool</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hi guys. Throwaway for obv reasons.\\n\\nI'm a f...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1741.0</td>\n",
       "      <td>335.0</td>\n",
       "      <td>Shitpost</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hi guys throwaway obv reasons i'm female child...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>556350346</td>\n",
       "      <td>t3_978ioa</td>\n",
       "      <td>1534254641</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Pauly104</td>\n",
       "      <td>AITA for eating steak in front of my vegan GF?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yesterday night, me and my GF decided to go ou...</td>\n",
       "      <td>1011.0</td>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>NaN</td>\n",
       "      <td>416.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>not the a-hole</td>\n",
       "      <td>not</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yesterday night gf decided eat vegan day going...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>560929656</td>\n",
       "      <td>t3_99yo3c</td>\n",
       "      <td>1535126620</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ThatSpencerGuy</td>\n",
       "      <td>AITA for not wanting to cook my mother-in-law ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My wife and I are vegetarians, much to my in-l...</td>\n",
       "      <td>349.0</td>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1158.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>not the a-hole</td>\n",
       "      <td>not</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wife vegetarians laws vocal annoyance year vis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       idint      idstr     created  self  nsfw          author  \\\n",
       "0  427576402  t3_72kg2a  1506433689   1.0   0.0          Ritsku   \n",
       "1  551887974  t3_94kvhi  1533404095   1.0   0.0   hhhhhhffff678   \n",
       "2  552654542  t3_951az2  1533562299   1.0   0.0  creepatthepool   \n",
       "3  556350346  t3_978ioa  1534254641   1.0   0.0        Pauly104   \n",
       "4  560929656  t3_99yo3c  1535126620   1.0   0.0  ThatSpencerGuy   \n",
       "\n",
       "                                               title  url  \\\n",
       "0  AITA for breaking up with my girlfriend becaus...  NaN   \n",
       "1  AITA for banning smoking in my house and telli...  NaN   \n",
       "2      AITA? Creep wears skimpy bathing suit to pool  NaN   \n",
       "3     AITA for eating steak in front of my vegan GF?  NaN   \n",
       "4  AITA for not wanting to cook my mother-in-law ...  NaN   \n",
       "\n",
       "                                            selftext   score      subreddit  \\\n",
       "0  My girlfriend recently went to the beach with ...   679.0  AmItheAsshole   \n",
       "1  My parents smoke like chimneys. I used to as w...   832.0  AmItheAsshole   \n",
       "2  Hi guys. Throwaway for obv reasons.\\n\\nI'm a f...    23.0  AmItheAsshole   \n",
       "3  Yesterday night, me and my GF decided to go ou...  1011.0  AmItheAsshole   \n",
       "4  My wife and I are vegetarians, much to my in-l...   349.0  AmItheAsshole   \n",
       "\n",
       "  distinguish  textlen  num_comments        flair_text flair_css_class  \\\n",
       "0         NaN   4917.0         434.0  no a--holes here             NaN   \n",
       "1         NaN   2076.0         357.0           asshole             ass   \n",
       "2         NaN   1741.0         335.0          Shitpost             NaN   \n",
       "3         NaN    416.0         380.0    not the a-hole             not   \n",
       "4         NaN   1158.0         360.0    not the a-hole             not   \n",
       "\n",
       "   augmented_at  augmented_count  \\\n",
       "0           NaN              NaN   \n",
       "1           NaN              NaN   \n",
       "2           NaN              NaN   \n",
       "3           NaN              NaN   \n",
       "4           NaN              NaN   \n",
       "\n",
       "                                             pp_text  \n",
       "0  girlfriend recently went beach friends tiny bi...  \n",
       "1  parents smoke like chimneys quit wife got youn...  \n",
       "2  hi guys throwaway obv reasons i'm female child...  \n",
       "3  yesterday night gf decided eat vegan day going...  \n",
       "4  wife vegetarians laws vocal annoyance year vis...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beach frie'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have a look at a slice of the first post\n",
    "df['pp_text'][0][25:35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eh5peeC6Fi6H"
   },
   "source": [
    "Once our phrase model has been trained on our total dataset, we can apply it to new text. When our model encounters two tokens in new text that identifies as a phrase, it will merge the two into a single new token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 188,
     "status": "ok",
     "timestamp": 1639846257823,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "FCR6DWQeFjt8",
    "outputId": "0c7b6e9c-ebe1-400d-e073-7c104aeddd02"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['That', 'was', 'not', 'a', 'big_deal']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_phraser[\"That\", \"was\", \"not\", \"a\", \"big\", \"deal\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the bigram parser. We can use `.keys()` to identify the bigrams in the dataset. How many bigrams were identified by the parser?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 216,
     "status": "ok",
     "timestamp": 1639846804300,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "xO4enOmLeXXi",
    "outputId": "921b6ce4-3390-4f90-ab13-e7bd4e0ce11f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "775"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bigram_phraser.phrasegrams.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the first few bigrams identified in the model as well to check if they seem like appropriate bigrams. If not, we can change the parameters of the bigram model to adjust the sensitivity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 346,
     "status": "ok",
     "timestamp": 1639846859895,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "uwySeLM4df97",
    "outputId": "0940755f-397e-48cc-ecdb-3558341cb7ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['worth_mentioning',\n",
       " 'big_deal',\n",
       " 'security_camera',\n",
       " 'mashed_potatoes',\n",
       " 'walking_eggshells',\n",
       " 'piece_shit',\n",
       " 'forgot_mention',\n",
       " 'high_school',\n",
       " 'outside_perspective',\n",
       " 'gain_weight']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bigram_phraser.phrasegrams.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blocked_social_media',\n",
       " 'social_media_accounts',\n",
       " 'long_story_short',\n",
       " 'mental_health_issues',\n",
       " 'upper_middle_class',\n",
       " 'giving_silent_treatment',\n",
       " 'bit_taken_aback',\n",
       " 'r_amitheasshole_comments',\n",
       " 'real_estate_agent',\n",
       " 'playing_video_games',\n",
       " 'plays_video_games',\n",
       " 'high_school_sweetheart',\n",
       " 'credit_card_debt',\n",
       " 'hell_broke_loose',\n",
       " 'posting_social_media',\n",
       " 'blah_blah_blah',\n",
       " 'rubbed_wrong_way',\n",
       " 'straw_broke_camel',\n",
       " 'play_video_games',\n",
       " 'posted_social_media',\n",
       " 'worst_case_scenario',\n",
       " 'giving_cold_shoulder',\n",
       " 'passive_aggressive_comments',\n",
       " '=_share&utm_medium',\n",
       " 'mobile_sorry_formatting',\n",
       " 'slammed_door_face',\n",
       " 'post_social_media',\n",
       " 'straw_broke_camels',\n",
       " 'aunts_uncles_cousins',\n",
       " 'monthly_open_forum',\n",
       " 'place_share_meta',\n",
       " 'dialog_mod_team.#keep',\n",
       " 'civil_rules_apply',\n",
       " 'discourage_brigading_needs',\n",
       " 'context_use_modmail',\n",
       " 'uncensored_screenshots_comments']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at trigrams\n",
    "[trigram for trigram in list(trigram_phraser.phrasegrams.keys()) if trigram.count('_') == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVgem81TZWzu"
   },
   "source": [
    "<a id='save'></a>\n",
    "\n",
    "# Saving Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save this cleaned-up dataframe in a new CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../../data/aita_pp.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running `to_csv()`, you'll see the data file gets added in the \"data\" folder, which is in the main folder of this repository. \n",
    "\n",
    "üîî **Question**: Can you find the data file using a file browser (either in Jupyter or on your machine)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "## ‚ùó Key Points\n",
    "\n",
    "* The Pandas `DataFrame` format can be used to save Reddit data.\n",
    "* Before cleaning text data, it is a good idea to drop rows and columns you don't need.\n",
    "* SpaCy can be used to preprocess textual data, including tokenization and lemmatization.\n",
    "* Gensim can be used to combine tokens into N-grams.\n",
    "* Python objects can be easily saved into binary format. This is called \"pickling\".\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Week 1 Preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dlab",
   "language": "python",
   "name": "dlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
